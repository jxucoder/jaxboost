{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"JAXBoost","text":"<p>Next-generation differentiable gradient boosting with JAX</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udf33 Soft Oblivious Trees - Differentiable tree structures with sigmoid routing</li> <li>\u2702\ufe0f Hyperplane Splits - Capture feature interactions beyond axis-aligned splits</li> <li>\ud83d\ude80 GPU-Efficient - Vectorized computation leveraging JAX's JIT compilation</li> <li>\ud83d\udd04 End-to-End Training - Gradient-based optimization via optax</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install jaxboost\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from jaxboost import GBMTrainer\n\n# Create trainer\ntrainer = GBMTrainer(task=\"regression\")\n\n# Fit model\nmodel = trainer.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with JAXBoost</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the JAXBoost API reference documentation.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>JAXBoost provides both high-level and low-level APIs:</p>"},{"location":"api/#high-level-api-recommended","title":"High-Level API (Recommended)","text":"<ul> <li><code>GBMTrainer</code> - Main training interface</li> <li><code>TrainerConfig</code> - Configuration options</li> </ul>"},{"location":"api/#low-level-components","title":"Low-Level Components","text":"<ul> <li>Splits - Split functions (axis-aligned, hyperplane)</li> <li>Structures - Tree structures (oblivious trees)</li> <li>Routing - Soft routing functions</li> <li>Aggregation - Boosting aggregation</li> <li>Losses - Loss functions</li> </ul>"},{"location":"api/#module-structure","title":"Module Structure","text":"<pre><code>jaxboost/\n\u251c\u2500\u2500 training/      # High-level training API\n\u251c\u2500\u2500 splits/        # Split mechanisms\n\u251c\u2500\u2500 structures/    # Tree structures\n\u251c\u2500\u2500 routing/       # Soft routing\n\u251c\u2500\u2500 aggregation/   # Ensemble aggregation\n\u2514\u2500\u2500 losses/        # Loss functions\n</code></pre>"},{"location":"api/aggregation/","title":"Aggregation","text":"<p>Boosting aggregation functions.</p>"},{"location":"api/aggregation/#jaxboost.aggregation.boosting","title":"boosting","text":"<p>Boosting aggregation.</p> <p>Combines tree predictions additively: f(x) = \u03a3 weight_t * tree_t(x)</p>"},{"location":"api/aggregation/#jaxboost.aggregation.boosting.boosting_aggregate","title":"boosting_aggregate","text":"<pre><code>boosting_aggregate(\n    tree_predictions: Array, weights: Array | None = None\n) -&gt; Array\n</code></pre> <p>Aggregate tree predictions using boosting (weighted sum).</p> <p>Parameters:</p> Name Type Description Default <code>tree_predictions</code> <code>Array</code> <p>Predictions from each tree, shape (num_trees, batch) or (num_trees,).</p> required <code>weights</code> <code>Array | None</code> <p>Optional weights for each tree, shape (num_trees,). If None, uniform weights (1.0) are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Aggregated predictions, shape (batch,) or scalar.</p> Source code in <code>src/jaxboost/aggregation/boosting.py</code> <pre><code>def boosting_aggregate(\n    tree_predictions: Array,\n    weights: Array | None = None,\n) -&gt; Array:\n    \"\"\"Aggregate tree predictions using boosting (weighted sum).\n\n    Args:\n        tree_predictions: Predictions from each tree,\n            shape (num_trees, batch) or (num_trees,).\n        weights: Optional weights for each tree, shape (num_trees,).\n            If None, uniform weights (1.0) are used.\n\n    Returns:\n        Aggregated predictions, shape (batch,) or scalar.\n    \"\"\"\n    if weights is None:\n        return jnp.sum(tree_predictions, axis=0)\n    else:\n        return jnp.einsum(\"t,t...-&gt;...\", weights, tree_predictions)\n</code></pre>"},{"location":"api/losses/","title":"Losses","text":"<p>Loss functions for training.</p>"},{"location":"api/losses/#regression-losses","title":"Regression Losses","text":""},{"location":"api/losses/#jaxboost.losses.regression","title":"regression","text":"<p>Regression loss functions.</p>"},{"location":"api/losses/#jaxboost.losses.regression.mse_loss","title":"mse_loss","text":"<pre><code>mse_loss(predictions: Array, targets: Array) -&gt; Array\n</code></pre> <p>Mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Array</code> <p>Model predictions, shape (batch,).</p> required <code>targets</code> <code>Array</code> <p>Ground truth targets, shape (batch,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar mean squared error.</p> Source code in <code>src/jaxboost/losses/regression.py</code> <pre><code>def mse_loss(predictions: Array, targets: Array) -&gt; Array:\n    \"\"\"Mean squared error loss.\n\n    Args:\n        predictions: Model predictions, shape (batch,).\n        targets: Ground truth targets, shape (batch,).\n\n    Returns:\n        Scalar mean squared error.\n    \"\"\"\n    return jnp.mean((predictions - targets) ** 2)\n</code></pre>"},{"location":"api/losses/#classification-losses","title":"Classification Losses","text":""},{"location":"api/losses/#jaxboost.losses.classification","title":"classification","text":"<p>Classification loss functions.</p>"},{"location":"api/losses/#jaxboost.losses.classification.sigmoid_binary_cross_entropy","title":"sigmoid_binary_cross_entropy","text":"<pre><code>sigmoid_binary_cross_entropy(\n    logits: Array, targets: Array\n) -&gt; Array\n</code></pre> <p>Binary cross-entropy loss with logits (numerically stable).</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>Raw logits (before sigmoid), shape (batch,).</p> required <code>targets</code> <code>Array</code> <p>Binary targets (0 or 1), shape (batch,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar binary cross-entropy loss.</p> Source code in <code>src/jaxboost/losses/classification.py</code> <pre><code>def sigmoid_binary_cross_entropy(logits: Array, targets: Array) -&gt; Array:\n    \"\"\"Binary cross-entropy loss with logits (numerically stable).\n\n    Args:\n        logits: Raw logits (before sigmoid), shape (batch,).\n        targets: Binary targets (0 or 1), shape (batch,).\n\n    Returns:\n        Scalar binary cross-entropy loss.\n    \"\"\"\n    # Numerically stable BCE:\n    # max(logits, 0) - logits * targets + log(1 + exp(-|logits|))\n    loss = (\n        jnp.maximum(logits, 0)\n        - logits * targets\n        + jnp.log1p(jnp.exp(-jnp.abs(logits)))\n    )\n    return jnp.mean(loss)\n</code></pre>"},{"location":"api/routing/","title":"Routing","text":"<p>Soft routing functions for differentiable trees.</p>"},{"location":"api/routing/#jaxboost.routing.soft","title":"soft","text":"<p>Soft (sigmoid) routing function.</p> <p>Converts split scores to probabilities using sigmoid function. This enables gradient flow through the tree structure.</p>"},{"location":"api/routing/#jaxboost.routing.soft.soft_routing","title":"soft_routing","text":"<pre><code>soft_routing(\n    score: Array, temperature: float = 1.0\n) -&gt; Array\n</code></pre> <p>Soft routing using sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Array</code> <p>Split scores, any shape.</p> required <code>temperature</code> <code>float</code> <p>Sharpness parameter. Higher = softer, Lower = sharper.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Probability of going right, same shape as score, values in [0, 1].</p> Source code in <code>src/jaxboost/routing/soft.py</code> <pre><code>def soft_routing(score: Array, temperature: float = 1.0) -&gt; Array:\n    \"\"\"Soft routing using sigmoid function.\n\n    Args:\n        score: Split scores, any shape.\n        temperature: Sharpness parameter. Higher = softer, Lower = sharper.\n\n    Returns:\n        Probability of going right, same shape as score, values in [0, 1].\n    \"\"\"\n    return jax.nn.sigmoid(score * temperature)\n</code></pre>"},{"location":"api/splits/","title":"Splits","text":"<p>Split mechanisms for decision trees.</p>"},{"location":"api/splits/#axis-aligned-splits","title":"Axis-Aligned Splits","text":""},{"location":"api/splits/#jaxboost.splits.axis_aligned","title":"axis_aligned","text":"<p>Axis-aligned split function (soft version).</p> <p>Uses softmax for differentiable feature selection.</p>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit","title":"AxisAlignedSplit","text":"<p>Soft axis-aligned split using softmax feature selection.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>class AxisAlignedSplit:\n    \"\"\"Soft axis-aligned split using softmax feature selection.\"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        init_threshold_scale: float = 0.1,\n    ) -&gt; AxisAlignedSplitParams:\n        \"\"\"Initialize split parameters.\"\"\"\n        keys = jax.random.split(key, 2)\n        feature_logits = jax.random.normal(keys[0], (num_features,)) * 0.01\n        threshold = jax.random.normal(keys[1], ()) * init_threshold_scale\n        return AxisAlignedSplitParams(feature_logits=feature_logits, threshold=threshold)\n\n    def compute_score(self, params: AxisAlignedSplitParams, x: Array) -&gt; Array:\n        \"\"\"Compute split score. Positive \u2192 right, Negative \u2192 left.\"\"\"\n        feature_probs = jax.nn.softmax(params.feature_logits)\n        selected_value = jnp.sum(x * feature_probs, axis=-1)\n        return selected_value - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    num_features: int,\n    init_threshold_scale: float = 0.1,\n) -&gt; AxisAlignedSplitParams\n</code></pre> <p>Initialize split parameters.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    init_threshold_scale: float = 0.1,\n) -&gt; AxisAlignedSplitParams:\n    \"\"\"Initialize split parameters.\"\"\"\n    keys = jax.random.split(key, 2)\n    feature_logits = jax.random.normal(keys[0], (num_features,)) * 0.01\n    threshold = jax.random.normal(keys[1], ()) * init_threshold_scale\n    return AxisAlignedSplitParams(feature_logits=feature_logits, threshold=threshold)\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit.compute_score","title":"compute_score","text":"<pre><code>compute_score(\n    params: AxisAlignedSplitParams, x: Array\n) -&gt; Array\n</code></pre> <p>Compute split score. Positive \u2192 right, Negative \u2192 left.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>def compute_score(self, params: AxisAlignedSplitParams, x: Array) -&gt; Array:\n    \"\"\"Compute split score. Positive \u2192 right, Negative \u2192 left.\"\"\"\n    feature_probs = jax.nn.softmax(params.feature_logits)\n    selected_value = jnp.sum(x * feature_probs, axis=-1)\n    return selected_value - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplitParams","title":"AxisAlignedSplitParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for a soft axis-aligned split.</p> <p>Attributes:</p> Name Type Description <code>feature_logits</code> <code>Array</code> <p>Soft feature selection logits, shape (num_features,).</p> <code>threshold</code> <code>Array</code> <p>Split threshold, scalar.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>class AxisAlignedSplitParams(NamedTuple):\n    \"\"\"Parameters for a soft axis-aligned split.\n\n    Attributes:\n        feature_logits: Soft feature selection logits, shape (num_features,).\n        threshold: Split threshold, scalar.\n    \"\"\"\n\n    feature_logits: Array  # (num_features,)\n    threshold: Array  # scalar\n</code></pre>"},{"location":"api/splits/#hyperplane-splits","title":"Hyperplane Splits","text":""},{"location":"api/splits/#jaxboost.splits.hyperplane","title":"hyperplane","text":"<p>Hyperplane split function.</p> <p>Instead of axis-aligned splits (x[j] &lt;= t), hyperplane splits use a learned linear combination: w @ x &lt;= t</p> <p>This captures feature interactions directly in a single split!</p>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit","title":"HyperplaneSplit","text":"<p>Hyperplane split: w @ x - threshold.</p> <p>More expressive than axis-aligned splits. A single hyperplane split can capture linear feature interactions.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>class HyperplaneSplit:\n    \"\"\"Hyperplane split: w @ x - threshold.\n\n    More expressive than axis-aligned splits.\n    A single hyperplane split can capture linear feature interactions.\n    \"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        init_scale: float = 0.1,\n    ) -&gt; HyperplaneSplitParams:\n        \"\"\"Initialize split parameters.\n\n        Weights are initialized small and normalized.\n        \"\"\"\n        keys = jax.random.split(key, 2)\n        weights = jax.random.normal(keys[0], (num_features,)) * init_scale\n        weights = weights / (jnp.linalg.norm(weights) + 1e-8)\n        threshold = jax.random.normal(keys[1], ()) * 0.1\n        return HyperplaneSplitParams(weights=weights, threshold=threshold)\n\n    def compute_score(self, params: HyperplaneSplitParams, x: Array) -&gt; Array:\n        \"\"\"Compute split score: w @ x - threshold.\n\n        Positive \u2192 go right, Negative \u2192 go left.\n        \"\"\"\n        # x: (..., num_features), weights: (num_features,)\n        projection = jnp.sum(x * params.weights, axis=-1)\n        return projection - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array, num_features: int, init_scale: float = 0.1\n) -&gt; HyperplaneSplitParams\n</code></pre> <p>Initialize split parameters.</p> <p>Weights are initialized small and normalized.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    init_scale: float = 0.1,\n) -&gt; HyperplaneSplitParams:\n    \"\"\"Initialize split parameters.\n\n    Weights are initialized small and normalized.\n    \"\"\"\n    keys = jax.random.split(key, 2)\n    weights = jax.random.normal(keys[0], (num_features,)) * init_scale\n    weights = weights / (jnp.linalg.norm(weights) + 1e-8)\n    threshold = jax.random.normal(keys[1], ()) * 0.1\n    return HyperplaneSplitParams(weights=weights, threshold=threshold)\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit.compute_score","title":"compute_score","text":"<pre><code>compute_score(\n    params: HyperplaneSplitParams, x: Array\n) -&gt; Array\n</code></pre> <p>Compute split score: w @ x - threshold.</p> <p>Positive \u2192 go right, Negative \u2192 go left.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>def compute_score(self, params: HyperplaneSplitParams, x: Array) -&gt; Array:\n    \"\"\"Compute split score: w @ x - threshold.\n\n    Positive \u2192 go right, Negative \u2192 go left.\n    \"\"\"\n    # x: (..., num_features), weights: (num_features,)\n    projection = jnp.sum(x * params.weights, axis=-1)\n    return projection - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplitParams","title":"HyperplaneSplitParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for a hyperplane split.</p> <p>Attributes:</p> Name Type Description <code>weights</code> <code>Array</code> <p>Linear combination weights, shape (num_features,).</p> <code>threshold</code> <code>Array</code> <p>Split threshold, scalar.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>class HyperplaneSplitParams(NamedTuple):\n    \"\"\"Parameters for a hyperplane split.\n\n    Attributes:\n        weights: Linear combination weights, shape (num_features,).\n        threshold: Split threshold, scalar.\n    \"\"\"\n\n    weights: Array  # (num_features,)\n    threshold: Array  # scalar\n</code></pre>"},{"location":"api/structures/","title":"Structures","text":"<p>Tree structures for gradient boosting.</p>"},{"location":"api/structures/#oblivious-trees","title":"Oblivious Trees","text":""},{"location":"api/structures/#jaxboost.structures.oblivious","title":"oblivious","text":"<p>Oblivious tree structure.</p> <p>An oblivious tree uses the same split at each depth level. This makes it highly efficient for GPU computation.</p> <p>Key property: For depth D, there are exactly 2^D leaves.</p>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree","title":"ObliviousTree","text":"<p>Oblivious (symmetric) tree structure.</p> <p>All nodes at the same depth share the same split. This enables efficient vectorized computation on GPU.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>class ObliviousTree:\n    \"\"\"Oblivious (symmetric) tree structure.\n\n    All nodes at the same depth share the same split.\n    This enables efficient vectorized computation on GPU.\n    \"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        depth: int,\n        num_features: int,\n        split_fn: SplitFn[Any],\n        init_leaf_scale: float = 0.01,\n    ) -&gt; ObliviousTreeParams:\n        \"\"\"Initialize tree parameters.\n\n        Args:\n            key: JAX PRNG key.\n            depth: Tree depth (number of split levels).\n            num_features: Number of input features.\n            split_fn: Split function to use.\n            init_leaf_scale: Scale for leaf value initialization.\n\n        Returns:\n            Initialized tree parameters.\n        \"\"\"\n        num_leaves = 2**depth\n        keys = jax.random.split(key, depth + 1)\n\n        split_params = [\n            split_fn.init_params(keys[d], num_features) for d in range(depth)\n        ]\n        leaf_values = jax.random.normal(keys[depth], (num_leaves,)) * init_leaf_scale\n\n        return ObliviousTreeParams(split_params=split_params, leaf_values=leaf_values)\n\n    def forward(\n        self,\n        params: ObliviousTreeParams,\n        x: Array,\n        split_fn: SplitFn[Any],\n        routing_fn: RoutingFn,\n    ) -&gt; Array:\n        \"\"\"Forward pass through the tree.\n\n        Args:\n            params: Tree parameters.\n            x: Input features, shape (batch, num_features) or (num_features,).\n            split_fn: Split function.\n            routing_fn: Routing function.\n\n        Returns:\n            Predictions, shape (batch,) or scalar.\n        \"\"\"\n        single_sample = x.ndim == 1\n        if single_sample:\n            x = x[None, :]\n\n        depth = len(params.split_params)\n        p_rights = []\n        for d in range(depth):\n            score = split_fn.compute_score(params.split_params[d], x)\n            p_rights.append(routing_fn(score))\n        p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n        leaf_probs = self._compute_leaf_probs(p_rights, depth)\n        output = jnp.sum(leaf_probs * params.leaf_values, axis=-1)\n\n        return output[0] if single_sample else output\n\n    def _compute_leaf_probs(self, p_rights: Array, depth: int) -&gt; Array:\n        \"\"\"Compute probability of reaching each leaf (fully vectorized).\n\n        Uses bit manipulation + broadcasting - no Python loops!\n\n        Each leaf index's binary representation encodes the path:\n        bit d = 0 means go left, bit d = 1 means go right at depth d.\n        \"\"\"\n        num_leaves = 2**depth\n\n        # Create binary mask: which way to go at each depth for each leaf\n        leaf_indices = jnp.arange(num_leaves)\n        depth_indices = jnp.arange(depth)\n\n        # bit_mask[leaf, depth] = 1 if leaf goes right at depth, else 0\n        bit_mask = (leaf_indices[:, None] &gt;&gt; depth_indices) &amp; 1  # (num_leaves, depth)\n\n        # p_rights: (depth, batch) -&gt; (batch, depth)\n        p_rights_T = p_rights.T  # (batch, depth)\n\n        # Broadcast: (batch, 1, depth) vs (1, num_leaves, depth)\n        p_rights_expanded = p_rights_T[:, None, :]  # (batch, 1, depth)\n        bit_mask_expanded = bit_mask[None, :, :]  # (1, num_leaves, depth)\n\n        # routing_prob = p_right if go_right else (1 - p_right)\n        routing_probs = (\n            bit_mask_expanded * p_rights_expanded\n            + (1 - bit_mask_expanded) * (1 - p_rights_expanded)\n        )  # (batch, num_leaves, depth)\n\n        # Product over depth dimension\n        leaf_probs = jnp.prod(routing_probs, axis=-1)  # (batch, num_leaves)\n\n        return leaf_probs\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    depth: int,\n    num_features: int,\n    split_fn: SplitFn[Any],\n    init_leaf_scale: float = 0.01,\n) -&gt; ObliviousTreeParams\n</code></pre> <p>Initialize tree parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>depth</code> <code>int</code> <p>Tree depth (number of split levels).</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>split_fn</code> <code>SplitFn[Any]</code> <p>Split function to use.</p> required <code>init_leaf_scale</code> <code>float</code> <p>Scale for leaf value initialization.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>ObliviousTreeParams</code> <p>Initialized tree parameters.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    depth: int,\n    num_features: int,\n    split_fn: SplitFn[Any],\n    init_leaf_scale: float = 0.01,\n) -&gt; ObliviousTreeParams:\n    \"\"\"Initialize tree parameters.\n\n    Args:\n        key: JAX PRNG key.\n        depth: Tree depth (number of split levels).\n        num_features: Number of input features.\n        split_fn: Split function to use.\n        init_leaf_scale: Scale for leaf value initialization.\n\n    Returns:\n        Initialized tree parameters.\n    \"\"\"\n    num_leaves = 2**depth\n    keys = jax.random.split(key, depth + 1)\n\n    split_params = [\n        split_fn.init_params(keys[d], num_features) for d in range(depth)\n    ]\n    leaf_values = jax.random.normal(keys[depth], (num_leaves,)) * init_leaf_scale\n\n    return ObliviousTreeParams(split_params=split_params, leaf_values=leaf_values)\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree.forward","title":"forward","text":"<pre><code>forward(\n    params: ObliviousTreeParams,\n    x: Array,\n    split_fn: SplitFn[Any],\n    routing_fn: RoutingFn,\n) -&gt; Array\n</code></pre> <p>Forward pass through the tree.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ObliviousTreeParams</code> <p>Tree parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features) or (num_features,).</p> required <code>split_fn</code> <code>SplitFn[Any]</code> <p>Split function.</p> required <code>routing_fn</code> <code>RoutingFn</code> <p>Routing function.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Predictions, shape (batch,) or scalar.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>def forward(\n    self,\n    params: ObliviousTreeParams,\n    x: Array,\n    split_fn: SplitFn[Any],\n    routing_fn: RoutingFn,\n) -&gt; Array:\n    \"\"\"Forward pass through the tree.\n\n    Args:\n        params: Tree parameters.\n        x: Input features, shape (batch, num_features) or (num_features,).\n        split_fn: Split function.\n        routing_fn: Routing function.\n\n    Returns:\n        Predictions, shape (batch,) or scalar.\n    \"\"\"\n    single_sample = x.ndim == 1\n    if single_sample:\n        x = x[None, :]\n\n    depth = len(params.split_params)\n    p_rights = []\n    for d in range(depth):\n        score = split_fn.compute_score(params.split_params[d], x)\n        p_rights.append(routing_fn(score))\n    p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n    leaf_probs = self._compute_leaf_probs(p_rights, depth)\n    output = jnp.sum(leaf_probs * params.leaf_values, axis=-1)\n\n    return output[0] if single_sample else output\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTreeParams","title":"ObliviousTreeParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for an oblivious tree.</p> <p>Attributes:</p> Name Type Description <code>split_params</code> <code>list[Any]</code> <p>Split parameters, one per depth level.</p> <code>leaf_values</code> <code>Array</code> <p>Prediction values at leaves, shape (2^depth,).</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>class ObliviousTreeParams(NamedTuple):\n    \"\"\"Parameters for an oblivious tree.\n\n    Attributes:\n        split_params: Split parameters, one per depth level.\n        leaf_values: Prediction values at leaves, shape (2^depth,).\n    \"\"\"\n\n    split_params: list[Any]  # length = depth\n    leaf_values: Array  # (2^depth,)\n</code></pre>"},{"location":"api/training/","title":"Training","text":"<p>High-level training API for JAXBoost.</p>"},{"location":"api/training/#jaxboost.training","title":"training","text":"<p>Training utilities for jaxboost.</p>"},{"location":"api/training/#jaxboost.training.GBMTrainer","title":"GBMTrainer","text":"<p>Gradient boosting trainer with soft oblivious trees.</p> Example <p>trainer = GBMTrainer(task=\"regression\") model = trainer.fit(X_train, y_train) predictions = model.predict(X_test)</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>class GBMTrainer:\n    \"\"\"Gradient boosting trainer with soft oblivious trees.\n\n    Example:\n        &gt;&gt;&gt; trainer = GBMTrainer(task=\"regression\")\n        &gt;&gt;&gt; model = trainer.fit(X_train, y_train)\n        &gt;&gt;&gt; predictions = model.predict(X_test)\n    \"\"\"\n\n    def __init__(\n        self,\n        task: Literal[\"regression\", \"classification\"] = \"regression\",\n        config: TrainerConfig | None = None,\n    ):\n        \"\"\"Initialize trainer.\n\n        Args:\n            task: 'regression' or 'classification'.\n            config: Training configuration. If None, uses defaults.\n        \"\"\"\n        self.task = task\n        self.config = config or TrainerConfig()\n        self.split_fn = HyperplaneSplit()\n        self.tree = ObliviousTree()\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        X_val: np.ndarray | None = None,\n        y_val: np.ndarray | None = None,\n    ) -&gt; TrainedGBM:\n        \"\"\"Train boosted trees on the given data.\n\n        Args:\n            X: Training features, shape (n_samples, n_features).\n            y: Training targets, shape (n_samples,).\n            X_val: Optional validation features.\n            y_val: Optional validation targets.\n\n        Returns:\n            Trained model.\n        \"\"\"\n        config = self.config\n        n_samples, n_features = X.shape\n\n        # Auto-adjust hyperparameters based on data size\n        config = self._adapt_config(config, n_samples)\n\n        # Create validation split if not provided\n        if X_val is None:\n            split_idx = int(n_samples * (1 - config.val_fraction))\n            indices = np.random.RandomState(42).permutation(n_samples)\n            train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n        else:\n            X_train, y_train = X, y\n\n        # Normalize target for regression\n        if self.task == \"regression\" and config.normalize_target:\n            target_mean, target_std = float(y_train.mean()), float(y_train.std())\n            y_train = (y_train - target_mean) / target_std\n            y_val = (y_val - target_mean) / target_std\n        else:\n            target_mean, target_std = 0.0, 1.0\n\n        # Convert to JAX arrays\n        X_train = jnp.array(X_train, dtype=jnp.float32)\n        y_train = jnp.array(y_train, dtype=jnp.float32)\n        X_val = jnp.array(X_val, dtype=jnp.float32)\n        y_val = jnp.array(y_val, dtype=jnp.float32)\n\n        # Initialize trees\n        key = jax.random.PRNGKey(42)\n        ensemble_params = []\n        for _ in range(config.n_trees):\n            key, subkey = jax.random.split(key)\n            params = self.tree.init_params(\n                subkey, config.depth, n_features, self.split_fn\n            )\n            ensemble_params.append(params)\n\n        # Setup optimizer\n        schedule = optax.cosine_decay_schedule(\n            config.learning_rate, config.epochs, alpha=0.01\n        )\n        optimizer = optax.adam(schedule)\n        opt_state = optimizer.init(ensemble_params)\n\n        # Training loop with early stopping\n        best_val_loss = float('inf')\n        best_params = ensemble_params\n        no_improve = 0\n\n        @jax.jit\n        def train_step(params, opt_state, X, y, temperature):\n            loss, grads = jax.value_and_grad(self._loss_fn)(\n                params, X, y, temperature, config\n            )\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss\n\n        for epoch in range(config.epochs):\n            progress = epoch / config.epochs\n            temperature = config.temp_start + progress * (config.temp_end - config.temp_start)\n\n            ensemble_params, opt_state, train_loss = train_step(\n                ensemble_params, opt_state, X_train, y_train, temperature\n            )\n\n            # Validation check\n            if epoch % 10 == 0:\n                val_loss = self._loss_fn(\n                    ensemble_params, X_val, y_val, temperature, config\n                )\n\n                if val_loss &lt; best_val_loss:\n                    best_val_loss = val_loss\n                    best_params = ensemble_params\n                    no_improve = 0\n                else:\n                    no_improve += 10\n\n                if config.verbose and epoch % 50 == 0:\n                    print(f\"  Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n\n                if no_improve &gt;= config.patience:\n                    if config.verbose:\n                        print(f\"  Early stopping at epoch {epoch}\")\n                    break\n\n        return TrainedGBM(\n            params=best_params,\n            config=config,\n            target_mean=target_mean,\n            target_std=target_std,\n            task=self.task,\n            split_fn=self.split_fn,\n            tree=self.tree,\n        )\n\n    def _adapt_config(self, config: TrainerConfig, n_samples: int) -&gt; TrainerConfig:\n        \"\"\"Adapt configuration based on dataset size.\"\"\"\n        if config.temp_end is None:\n            if n_samples &lt; 500:\n                temp_end = 3.0\n            elif n_samples &lt; 5000:\n                temp_end = 5.0\n            else:\n                temp_end = 10.0\n        else:\n            temp_end = config.temp_end\n\n        depth = config.depth\n        lr = config.learning_rate\n        if n_samples &lt; 500:\n            depth = min(depth, 3)\n            lr = min(lr, 0.01)\n\n        return TrainerConfig(\n            n_trees=config.n_trees,\n            depth=depth,\n            learning_rate=lr,\n            tree_weight=config.tree_weight,\n            epochs=config.epochs,\n            patience=config.patience,\n            temp_start=config.temp_start,\n            temp_end=temp_end,\n            val_fraction=config.val_fraction,\n            normalize_target=config.normalize_target,\n            verbose=config.verbose,\n        )\n\n    def _loss_fn(\n        self,\n        params: list,\n        X: Array,\n        y: Array,\n        temperature: float,\n        config: TrainerConfig,\n    ) -&gt; Array:\n        \"\"\"Compute loss for the ensemble.\"\"\"\n        def routing_fn(score):\n            return soft_routing(score, temperature)\n\n        preds = []\n        for tree_params in params:\n            pred = self.tree.forward(tree_params, X, self.split_fn, routing_fn)\n            preds.append(pred)\n\n        preds = jnp.stack(preds, axis=0)\n        weights = jnp.full((config.n_trees,), config.tree_weight)\n        output = boosting_aggregate(preds, weights)\n\n        if self.task == \"regression\":\n            return mse_loss(output, y)\n        else:\n            return sigmoid_binary_cross_entropy(output, y)\n</code></pre>"},{"location":"api/training/#jaxboost.training.GBMTrainer.__init__","title":"__init__","text":"<pre><code>__init__(\n    task: Literal[\n        \"regression\", \"classification\"\n    ] = \"regression\",\n    config: TrainerConfig | None = None,\n)\n</code></pre> <p>Initialize trainer.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Literal['regression', 'classification']</code> <p>'regression' or 'classification'.</p> <code>'regression'</code> <code>config</code> <code>TrainerConfig | None</code> <p>Training configuration. If None, uses defaults.</p> <code>None</code> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>def __init__(\n    self,\n    task: Literal[\"regression\", \"classification\"] = \"regression\",\n    config: TrainerConfig | None = None,\n):\n    \"\"\"Initialize trainer.\n\n    Args:\n        task: 'regression' or 'classification'.\n        config: Training configuration. If None, uses defaults.\n    \"\"\"\n    self.task = task\n    self.config = config or TrainerConfig()\n    self.split_fn = HyperplaneSplit()\n    self.tree = ObliviousTree()\n</code></pre>"},{"location":"api/training/#jaxboost.training.GBMTrainer.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    X_val: ndarray | None = None,\n    y_val: ndarray | None = None,\n) -&gt; TrainedGBM\n</code></pre> <p>Train boosted trees on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training features, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> required <code>X_val</code> <code>ndarray | None</code> <p>Optional validation features.</p> <code>None</code> <code>y_val</code> <code>ndarray | None</code> <p>Optional validation targets.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainedGBM</code> <p>Trained model.</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    X_val: np.ndarray | None = None,\n    y_val: np.ndarray | None = None,\n) -&gt; TrainedGBM:\n    \"\"\"Train boosted trees on the given data.\n\n    Args:\n        X: Training features, shape (n_samples, n_features).\n        y: Training targets, shape (n_samples,).\n        X_val: Optional validation features.\n        y_val: Optional validation targets.\n\n    Returns:\n        Trained model.\n    \"\"\"\n    config = self.config\n    n_samples, n_features = X.shape\n\n    # Auto-adjust hyperparameters based on data size\n    config = self._adapt_config(config, n_samples)\n\n    # Create validation split if not provided\n    if X_val is None:\n        split_idx = int(n_samples * (1 - config.val_fraction))\n        indices = np.random.RandomState(42).permutation(n_samples)\n        train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n    else:\n        X_train, y_train = X, y\n\n    # Normalize target for regression\n    if self.task == \"regression\" and config.normalize_target:\n        target_mean, target_std = float(y_train.mean()), float(y_train.std())\n        y_train = (y_train - target_mean) / target_std\n        y_val = (y_val - target_mean) / target_std\n    else:\n        target_mean, target_std = 0.0, 1.0\n\n    # Convert to JAX arrays\n    X_train = jnp.array(X_train, dtype=jnp.float32)\n    y_train = jnp.array(y_train, dtype=jnp.float32)\n    X_val = jnp.array(X_val, dtype=jnp.float32)\n    y_val = jnp.array(y_val, dtype=jnp.float32)\n\n    # Initialize trees\n    key = jax.random.PRNGKey(42)\n    ensemble_params = []\n    for _ in range(config.n_trees):\n        key, subkey = jax.random.split(key)\n        params = self.tree.init_params(\n            subkey, config.depth, n_features, self.split_fn\n        )\n        ensemble_params.append(params)\n\n    # Setup optimizer\n    schedule = optax.cosine_decay_schedule(\n        config.learning_rate, config.epochs, alpha=0.01\n    )\n    optimizer = optax.adam(schedule)\n    opt_state = optimizer.init(ensemble_params)\n\n    # Training loop with early stopping\n    best_val_loss = float('inf')\n    best_params = ensemble_params\n    no_improve = 0\n\n    @jax.jit\n    def train_step(params, opt_state, X, y, temperature):\n        loss, grads = jax.value_and_grad(self._loss_fn)(\n            params, X, y, temperature, config\n        )\n        updates, opt_state = optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss\n\n    for epoch in range(config.epochs):\n        progress = epoch / config.epochs\n        temperature = config.temp_start + progress * (config.temp_end - config.temp_start)\n\n        ensemble_params, opt_state, train_loss = train_step(\n            ensemble_params, opt_state, X_train, y_train, temperature\n        )\n\n        # Validation check\n        if epoch % 10 == 0:\n            val_loss = self._loss_fn(\n                ensemble_params, X_val, y_val, temperature, config\n            )\n\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_params = ensemble_params\n                no_improve = 0\n            else:\n                no_improve += 10\n\n            if config.verbose and epoch % 50 == 0:\n                print(f\"  Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n\n            if no_improve &gt;= config.patience:\n                if config.verbose:\n                    print(f\"  Early stopping at epoch {epoch}\")\n                break\n\n    return TrainedGBM(\n        params=best_params,\n        config=config,\n        target_mean=target_mean,\n        target_std=target_std,\n        task=self.task,\n        split_fn=self.split_fn,\n        tree=self.tree,\n    )\n</code></pre>"},{"location":"api/training/#jaxboost.training.TrainerConfig","title":"TrainerConfig  <code>dataclass</code>","text":"<p>Configuration for gradient boosting training.</p> <p>Attributes:</p> Name Type Description <code>n_trees</code> <code>int</code> <p>Number of trees in the ensemble.</p> <code>depth</code> <code>int</code> <p>Maximum depth of each tree.</p> <code>learning_rate</code> <code>float</code> <p>Base learning rate for optimizer.</p> <code>tree_weight</code> <code>float</code> <p>Weight for each tree's contribution.</p> <code>epochs</code> <code>int</code> <p>Maximum number of training epochs.</p> <code>patience</code> <code>int</code> <p>Early stopping patience (epochs without improvement).</p> <code>temp_start</code> <code>float</code> <p>Starting temperature for soft routing.</p> <code>temp_end</code> <code>float | None</code> <p>Ending temperature (None = auto based on data size).</p> <code>val_fraction</code> <code>float</code> <p>Fraction of training data for validation (if no val set provided).</p> <code>normalize_target</code> <code>bool</code> <p>Whether to normalize regression targets.</p> <code>verbose</code> <code>bool</code> <p>Whether to print training progress.</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>@dataclass\nclass TrainerConfig:\n    \"\"\"Configuration for gradient boosting training.\n\n    Attributes:\n        n_trees: Number of trees in the ensemble.\n        depth: Maximum depth of each tree.\n        learning_rate: Base learning rate for optimizer.\n        tree_weight: Weight for each tree's contribution.\n        epochs: Maximum number of training epochs.\n        patience: Early stopping patience (epochs without improvement).\n        temp_start: Starting temperature for soft routing.\n        temp_end: Ending temperature (None = auto based on data size).\n        val_fraction: Fraction of training data for validation (if no val set provided).\n        normalize_target: Whether to normalize regression targets.\n        verbose: Whether to print training progress.\n    \"\"\"\n    n_trees: int = 20\n    depth: int = 4\n    learning_rate: float = 0.01\n    tree_weight: float = 0.1\n    epochs: int = 500\n    patience: int = 50\n    temp_start: float = 1.0\n    temp_end: float | None = None  # Auto-detect\n    val_fraction: float = 0.15\n    normalize_target: bool = True\n    verbose: bool = False\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with JAXBoost.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install jaxboost\n</code></pre>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#regression","title":"Regression","text":"<pre><code>import jax.numpy as jnp\nfrom jaxboost import GBMTrainer\n\n# Generate sample data\nX_train = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\ny_train = jnp.array([1.0, 2.0, 3.0])\n\n# Create and fit trainer\ntrainer = GBMTrainer(task=\"regression\")\nmodel = trainer.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_train)\n</code></pre>"},{"location":"getting-started/quickstart/#binary-classification","title":"Binary Classification","text":"<pre><code>from jaxboost import GBMTrainer\n\ntrainer = GBMTrainer(task=\"binary_classification\")\nmodel = trainer.fit(X_train, y_train)\n\n# Get probability predictions\nprobabilities = model.predict_proba(X_train)\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Use <code>TrainerConfig</code> for advanced configuration:</p> <pre><code>from jaxboost import GBMTrainer, TrainerConfig\n\nconfig = TrainerConfig(\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n)\n\ntrainer = GBMTrainer(config=config, task=\"regression\")\n</code></pre>"}]}