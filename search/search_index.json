{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Next-generation differentiable gradient boosting with JAX</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udf33 Soft Oblivious Trees - Differentiable tree structures with sigmoid routing</li> <li>\u2702\ufe0f Hyperplane Splits - Capture feature interactions beyond axis-aligned splits</li> <li>\ud83d\ude80 GPU-Efficient - Vectorized computation leveraging JAX's JIT compilation</li> <li>\ud83d\udd04 End-to-End Training - Gradient-based optimization via optax</li> <li>\ud83c\udfaf Mixture of Experts - MOE with soft tree experts or XGBoost/LightGBM/CatBoost</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install jaxboost\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from jaxboost import GBMTrainer\n\n# Create trainer\ntrainer = GBMTrainer(task=\"regression\")\n\n# Fit model\nmodel = trainer.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with JAXBoost</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the JAXBoost API reference documentation.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>JAXBoost provides both high-level and low-level APIs:</p>"},{"location":"api/#high-level-api-recommended","title":"High-Level API (Recommended)","text":"<ul> <li><code>GBMTrainer</code> - Main training interface</li> <li><code>TrainerConfig</code> - Configuration options</li> </ul>"},{"location":"api/#ensemble-mixture-of-experts","title":"Ensemble &amp; Mixture of Experts","text":"<ul> <li><code>MOEEnsemble</code> - Differentiable MOE with soft tree experts</li> <li><code>EMMOE</code> - EM-trained MOE with XGBoost/LightGBM/CatBoost experts</li> <li>Gating Networks - Linear, MLP, and Tree gating</li> </ul>"},{"location":"api/#low-level-components","title":"Low-Level Components","text":"<ul> <li>Splits - Split functions (axis-aligned, hyperplane)</li> <li>Structures - Tree structures (oblivious trees)</li> <li>Routing - Soft routing functions</li> <li>Aggregation - Boosting aggregation</li> <li>Losses - Loss functions</li> </ul>"},{"location":"api/#module-structure","title":"Module Structure","text":"<pre><code>jaxboost/\n\u251c\u2500\u2500 training/      # High-level training API\n\u251c\u2500\u2500 ensemble/      # MOE architectures (differentiable &amp; hybrid)\n\u251c\u2500\u2500 splits/        # Split mechanisms\n\u251c\u2500\u2500 structures/    # Tree structures\n\u251c\u2500\u2500 routing/       # Soft routing\n\u251c\u2500\u2500 aggregation/   # Ensemble aggregation\n\u2514\u2500\u2500 losses/        # Loss functions\n</code></pre>"},{"location":"api/aggregation/","title":"Aggregation","text":"<p>Boosting aggregation functions.</p>"},{"location":"api/aggregation/#jaxboost.aggregation.boosting","title":"boosting","text":"<p>Boosting aggregation.</p> <p>Combines tree predictions additively: f(x) = \u03a3 weight_t * tree_t(x)</p>"},{"location":"api/aggregation/#jaxboost.aggregation.boosting.boosting_aggregate","title":"boosting_aggregate","text":"<pre><code>boosting_aggregate(\n    tree_predictions: Array, weights: Array | None = None\n) -&gt; Array\n</code></pre> <p>Aggregate tree predictions using boosting (weighted sum).</p> <p>Parameters:</p> Name Type Description Default <code>tree_predictions</code> <code>Array</code> <p>Predictions from each tree, shape (num_trees, batch) or (num_trees,).</p> required <code>weights</code> <code>Array | None</code> <p>Optional weights for each tree, shape (num_trees,). If None, uniform weights (1.0) are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Aggregated predictions, shape (batch,) or scalar.</p> Source code in <code>src/jaxboost/aggregation/boosting.py</code> <pre><code>def boosting_aggregate(\n    tree_predictions: Array,\n    weights: Array | None = None,\n) -&gt; Array:\n    \"\"\"Aggregate tree predictions using boosting (weighted sum).\n\n    Args:\n        tree_predictions: Predictions from each tree,\n            shape (num_trees, batch) or (num_trees,).\n        weights: Optional weights for each tree, shape (num_trees,).\n            If None, uniform weights (1.0) are used.\n\n    Returns:\n        Aggregated predictions, shape (batch,) or scalar.\n    \"\"\"\n    if weights is None:\n        return jnp.sum(tree_predictions, axis=0)\n    else:\n        return jnp.einsum(\"t,t...-&gt;...\", weights, tree_predictions)\n</code></pre>"},{"location":"api/ensemble/","title":"Ensemble &amp; Mixture of Experts","text":""},{"location":"api/ensemble/#differentiable-moe","title":"Differentiable MOE","text":"<p>End-to-end trainable MOE with soft decision tree experts.</p>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble","title":"MOEEnsemble","text":"<p>Mixture of Experts with GBDT experts.</p> Architecture <p>Input x \u2192 Gating Network \u2192 Expert weights g(x)         \u2192 Expert_1(x), Expert_2(x), ..., Expert_K(x)         \u2192 Output = \u03a3 g_k(x) * Expert_k(x)</p> <p>Each expert is a boosted ensemble of soft oblivious trees.</p> Example <p>moe = MOEEnsemble( ...     num_experts=4, ...     trees_per_expert=10, ...     tree_depth=4, ...     gating=\"tree\", ... )</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>class MOEEnsemble:\n    \"\"\"Mixture of Experts with GBDT experts.\n\n    Architecture:\n        Input x \u2192 Gating Network \u2192 Expert weights g(x)\n                \u2192 Expert_1(x), Expert_2(x), ..., Expert_K(x)\n                \u2192 Output = \u03a3 g_k(x) * Expert_k(x)\n\n    Each expert is a boosted ensemble of soft oblivious trees.\n\n    Example:\n        &gt;&gt;&gt; moe = MOEEnsemble(\n        ...     num_experts=4,\n        ...     trees_per_expert=10,\n        ...     tree_depth=4,\n        ...     gating=\"tree\",\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Initialize\n        &gt;&gt;&gt; key = jax.random.PRNGKey(42)\n        &gt;&gt;&gt; params = moe.init_params(key, num_features=10)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Forward pass\n        &gt;&gt;&gt; predictions = moe.forward(params, X)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Training\n        &gt;&gt;&gt; params = moe.fit(X_train, y_train)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_experts: int = 4,\n        trees_per_expert: int = 10,\n        tree_depth: int = 4,\n        tree_weight: float = 0.1,\n        gating: Literal[\"linear\", \"mlp\", \"tree\"] | GatingFn = \"tree\",\n        gating_temperature: float = 1.0,\n        top_k: int | None = None,\n        load_balance_weight: float = 0.01,\n        task: Literal[\"regression\", \"classification\"] = \"regression\",\n    ) -&gt; None:\n        \"\"\"Initialize MOE ensemble.\n\n        Args:\n            num_experts: Number of expert GBDTs.\n            trees_per_expert: Number of trees in each expert GBDT.\n            tree_depth: Depth of each tree.\n            tree_weight: Weight for each tree's contribution (learning rate).\n            gating: Gating type or custom GatingFn instance.\n                - \"linear\": LinearGating\n                - \"mlp\": MLPGating with hidden_dim=32\n                - \"tree\": TreeGating (depth auto-computed from num_experts)\n            gating_temperature: Temperature for gating softmax.\n            top_k: If set, only top-k experts are activated (sparse routing).\n            load_balance_weight: Weight for load balancing auxiliary loss.\n            task: \"regression\" or \"classification\".\n        \"\"\"\n        self.num_experts = num_experts\n        self.trees_per_expert = trees_per_expert\n        self.tree_depth = tree_depth\n        self.tree_weight = tree_weight\n        self.gating_temperature = gating_temperature\n        self.top_k = top_k\n        self.load_balance_weight = load_balance_weight\n        self.task = task\n\n        # Initialize gating\n        if isinstance(gating, str):\n            if gating == \"linear\":\n                self.gating = LinearGating()\n            elif gating == \"mlp\":\n                self.gating = MLPGating(hidden_dim=32)\n            elif gating == \"tree\":\n                # Compute depth from num_experts\n                tree_gating_depth = int(jnp.ceil(jnp.log2(num_experts)))\n                if 2 ** tree_gating_depth != num_experts:\n                    raise ValueError(\n                        f\"TreeGating requires num_experts to be a power of 2, \"\n                        f\"got {num_experts}. Use {2 ** tree_gating_depth} experts \"\n                        f\"or choose a different gating type.\"\n                    )\n                self.gating = TreeGating(depth=tree_gating_depth)\n            else:\n                raise ValueError(f\"Unknown gating type: {gating}\")\n        else:\n            self.gating = gating\n\n        # Tree components (shared across all experts)\n        self.split_fn = HyperplaneSplit()\n        self.tree = ObliviousTree()\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n    ) -&gt; MOEParams:\n        \"\"\"Initialize all parameters.\n\n        Args:\n            key: JAX PRNG key.\n            num_features: Number of input features.\n\n        Returns:\n            Initialized MOE parameters.\n        \"\"\"\n        keys = jax.random.split(key, self.num_experts + 1)\n\n        # Initialize gating\n        gating_params = self.gating.init_params(\n            keys[0], num_features, self.num_experts\n        )\n\n        # Initialize experts\n        expert_params = []\n        for k in range(self.num_experts):\n            expert_keys = jax.random.split(keys[k + 1], self.trees_per_expert)\n            trees = [\n                self.tree.init_params(\n                    expert_keys[t], \n                    self.tree_depth, \n                    num_features, \n                    self.split_fn,\n                )\n                for t in range(self.trees_per_expert)\n            ]\n            expert_params.append(trees)\n\n        return MOEParams(gating_params=gating_params, expert_params=expert_params)\n\n    def forward(\n        self,\n        params: MOEParams,\n        x: Array,\n        routing_temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Forward pass through MOE.\n\n        Args:\n            params: MOE parameters.\n            x: Input features, shape (batch, num_features).\n            routing_temperature: Temperature for soft routing in trees.\n\n        Returns:\n            Predictions, shape (batch,).\n        \"\"\"\n        single_sample = x.ndim == 1\n        if single_sample:\n            x = x[None, :]\n\n        # Compute gating weights\n        gate_weights = self.gating(\n            params.gating_params, x, self.gating_temperature\n        )  # (batch, num_experts)\n\n        # Apply sparse top-k if specified\n        if self.top_k is not None:\n            gate_weights, _ = sparse_top_k(gate_weights, self.top_k)\n\n        # Compute expert predictions\n        expert_preds = self._compute_expert_predictions(\n            params.expert_params, x, routing_temperature\n        )  # (batch, num_experts)\n\n        # Weighted combination\n        output = jnp.sum(gate_weights * expert_preds, axis=-1)  # (batch,)\n\n        return output[0] if single_sample else output\n\n    def _compute_expert_predictions(\n        self,\n        expert_params: list[list[Any]],\n        x: Array,\n        routing_temperature: float,\n    ) -&gt; Array:\n        \"\"\"Compute predictions from all experts.\n\n        Args:\n            expert_params: List of expert parameters.\n            x: Input features, shape (batch, num_features).\n            routing_temperature: Temperature for soft routing.\n\n        Returns:\n            Expert predictions, shape (batch, num_experts).\n        \"\"\"\n        def routing_fn(score):\n            return soft_routing(score, routing_temperature)\n\n        expert_preds = []\n        for k in range(self.num_experts):\n            # Each expert is a boosted ensemble\n            tree_preds = []\n            for tree_params in expert_params[k]:\n                pred = self.tree.forward(tree_params, x, self.split_fn, routing_fn)\n                tree_preds.append(pred)\n\n            tree_preds = jnp.stack(tree_preds, axis=0)  # (trees_per_expert, batch)\n            weights = jnp.full((self.trees_per_expert,), self.tree_weight)\n            expert_pred = boosting_aggregate(tree_preds, weights)  # (batch,)\n            expert_preds.append(expert_pred)\n\n        return jnp.stack(expert_preds, axis=-1)  # (batch, num_experts)\n\n    def loss(\n        self,\n        params: MOEParams,\n        x: Array,\n        y: Array,\n        routing_temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Compute total loss including auxiliary losses.\n\n        Args:\n            params: MOE parameters.\n            x: Input features, shape (batch, num_features).\n            y: Targets, shape (batch,).\n            routing_temperature: Temperature for soft routing.\n\n        Returns:\n            Total loss (scalar).\n        \"\"\"\n        # Forward pass\n        pred = self.forward(params, x, routing_temperature)\n\n        # Task loss\n        if self.task == \"regression\":\n            task_loss = mse_loss(pred, y)\n        else:\n            task_loss = sigmoid_binary_cross_entropy(pred, y)\n\n        # Load balancing loss\n        gate_weights = self.gating(\n            params.gating_params, x, self.gating_temperature\n        )\n        lb_loss = load_balance_loss(gate_weights)\n\n        return task_loss + self.load_balance_weight * lb_loss\n\n    def fit(\n        self,\n        X: Array,\n        y: Array,\n        X_val: Array | None = None,\n        y_val: Array | None = None,\n        epochs: int = 300,\n        learning_rate: float = 0.01,\n        temp_start: float = 1.0,\n        temp_end: float = 5.0,\n        patience: int = 50,\n        verbose: bool = True,\n    ) -&gt; MOEParams:\n        \"\"\"Train the MOE ensemble.\n\n        Args:\n            X: Training features, shape (n_samples, n_features).\n            y: Training targets, shape (n_samples,).\n            X_val: Validation features (optional).\n            y_val: Validation targets (optional).\n            epochs: Number of training epochs.\n            learning_rate: Learning rate for optimizer.\n            temp_start: Starting temperature for routing.\n            temp_end: Ending temperature for routing.\n            patience: Early stopping patience.\n            verbose: Whether to print progress.\n\n        Returns:\n            Trained parameters.\n        \"\"\"\n        X = jnp.array(X, dtype=jnp.float32)\n        y = jnp.array(y, dtype=jnp.float32)\n\n        # Create validation split if not provided\n        if X_val is None:\n            n_samples = X.shape[0]\n            split_idx = int(n_samples * 0.85)\n            indices = jax.random.permutation(jax.random.PRNGKey(42), n_samples)\n            train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n        else:\n            X_train, y_train = X, y\n            X_val = jnp.array(X_val, dtype=jnp.float32)\n            y_val = jnp.array(y_val, dtype=jnp.float32)\n\n        # Initialize\n        key = jax.random.PRNGKey(42)\n        num_features = X_train.shape[1]\n        params = self.init_params(key, num_features)\n\n        # Setup optimizer\n        schedule = optax.cosine_decay_schedule(learning_rate, epochs, alpha=0.01)\n        optimizer = optax.adam(schedule)\n        opt_state = optimizer.init(params)\n\n        # Training loop\n        best_val_loss = float('inf')\n        best_params = params\n        no_improve = 0\n\n        @jax.jit\n        def train_step(params, opt_state, temperature):\n            loss_val, grads = jax.value_and_grad(self.loss)(\n                params, X_train, y_train, temperature\n            )\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss_val\n\n        for epoch in range(epochs):\n            # Temperature annealing\n            progress = epoch / epochs\n            temperature = temp_start + progress * (temp_end - temp_start)\n\n            # Train step\n            params, opt_state, train_loss = train_step(params, opt_state, temperature)\n\n            # Validation\n            if epoch % 10 == 0:\n                val_loss = self.loss(params, X_val, y_val, temperature)\n\n                if val_loss &lt; best_val_loss:\n                    best_val_loss = val_loss\n                    best_params = params\n                    no_improve = 0\n                else:\n                    no_improve += 10\n\n                if verbose and epoch % 50 == 0:\n                    # Compute metrics\n                    pred_train = self.forward(params, X_train, temperature)\n                    pred_val = self.forward(params, X_val, temperature)\n\n                    if self.task == \"regression\":\n                        train_metric = jnp.mean((pred_train - y_train) ** 2)\n                        val_metric = jnp.mean((pred_val - y_val) ** 2)\n                        metric_name = \"MSE\"\n                    else:\n                        train_metric = jnp.mean((pred_train &gt; 0) == y_train)\n                        val_metric = jnp.mean((pred_val &gt; 0) == y_val)\n                        metric_name = \"Acc\"\n\n                    print(\n                        f\"Epoch {epoch}: loss={train_loss:.4f}, \"\n                        f\"train_{metric_name}={train_metric:.4f}, \"\n                        f\"val_{metric_name}={val_metric:.4f}\"\n                    )\n\n                if no_improve &gt;= patience:\n                    if verbose:\n                        print(f\"Early stopping at epoch {epoch}\")\n                    break\n\n        return best_params\n\n    def predict(\n        self,\n        params: MOEParams,\n        X: Array,\n        routing_temperature: float = 5.0,\n    ) -&gt; Array:\n        \"\"\"Make predictions.\n\n        Args:\n            params: Trained parameters.\n            X: Features, shape (n_samples, n_features).\n            routing_temperature: Temperature for routing (use trained temp).\n\n        Returns:\n            Predictions. For regression: values. For classification: probabilities.\n        \"\"\"\n        X = jnp.array(X, dtype=jnp.float32)\n        output = self.forward(params, X, routing_temperature)\n\n        if self.task == \"classification\":\n            return jax.nn.sigmoid(output)\n        return output\n\n    def get_expert_weights(\n        self,\n        params: MOEParams,\n        X: Array,\n    ) -&gt; Array:\n        \"\"\"Get expert weights for analysis.\n\n        Args:\n            params: MOE parameters.\n            X: Input features, shape (n_samples, n_features).\n\n        Returns:\n            Expert weights, shape (n_samples, num_experts).\n        \"\"\"\n        X = jnp.array(X, dtype=jnp.float32)\n        gate_weights = self.gating(\n            params.gating_params, X, self.gating_temperature\n        )\n        if self.top_k is not None:\n            gate_weights, _ = sparse_top_k(gate_weights, self.top_k)\n        return gate_weights\n\n    def get_expert_predictions(\n        self,\n        params: MOEParams,\n        X: Array,\n        routing_temperature: float = 5.0,\n    ) -&gt; Array:\n        \"\"\"Get individual expert predictions for analysis.\n\n        Args:\n            params: MOE parameters.\n            X: Input features, shape (n_samples, n_features).\n            routing_temperature: Temperature for routing.\n\n        Returns:\n            Expert predictions, shape (n_samples, num_experts).\n        \"\"\"\n        X = jnp.array(X, dtype=jnp.float32)\n        return self._compute_expert_predictions(\n            params.expert_params, X, routing_temperature\n        )\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble--initialize","title":"Initialize","text":"<p>key = jax.random.PRNGKey(42) params = moe.init_params(key, num_features=10)</p>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble--forward-pass","title":"Forward pass","text":"<p>predictions = moe.forward(params, X)</p>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble--training","title":"Training","text":"<p>params = moe.fit(X_train, y_train)</p>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_experts: int = 4,\n    trees_per_expert: int = 10,\n    tree_depth: int = 4,\n    tree_weight: float = 0.1,\n    gating: Literal[\"linear\", \"mlp\", \"tree\"]\n    | GatingFn = \"tree\",\n    gating_temperature: float = 1.0,\n    top_k: int | None = None,\n    load_balance_weight: float = 0.01,\n    task: Literal[\n        \"regression\", \"classification\"\n    ] = \"regression\",\n) -&gt; None\n</code></pre> <p>Initialize MOE ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of expert GBDTs.</p> <code>4</code> <code>trees_per_expert</code> <code>int</code> <p>Number of trees in each expert GBDT.</p> <code>10</code> <code>tree_depth</code> <code>int</code> <p>Depth of each tree.</p> <code>4</code> <code>tree_weight</code> <code>float</code> <p>Weight for each tree's contribution (learning rate).</p> <code>0.1</code> <code>gating</code> <code>Literal['linear', 'mlp', 'tree'] | GatingFn</code> <p>Gating type or custom GatingFn instance. - \"linear\": LinearGating - \"mlp\": MLPGating with hidden_dim=32 - \"tree\": TreeGating (depth auto-computed from num_experts)</p> <code>'tree'</code> <code>gating_temperature</code> <code>float</code> <p>Temperature for gating softmax.</p> <code>1.0</code> <code>top_k</code> <code>int | None</code> <p>If set, only top-k experts are activated (sparse routing).</p> <code>None</code> <code>load_balance_weight</code> <code>float</code> <p>Weight for load balancing auxiliary loss.</p> <code>0.01</code> <code>task</code> <code>Literal['regression', 'classification']</code> <p>\"regression\" or \"classification\".</p> <code>'regression'</code> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int = 4,\n    trees_per_expert: int = 10,\n    tree_depth: int = 4,\n    tree_weight: float = 0.1,\n    gating: Literal[\"linear\", \"mlp\", \"tree\"] | GatingFn = \"tree\",\n    gating_temperature: float = 1.0,\n    top_k: int | None = None,\n    load_balance_weight: float = 0.01,\n    task: Literal[\"regression\", \"classification\"] = \"regression\",\n) -&gt; None:\n    \"\"\"Initialize MOE ensemble.\n\n    Args:\n        num_experts: Number of expert GBDTs.\n        trees_per_expert: Number of trees in each expert GBDT.\n        tree_depth: Depth of each tree.\n        tree_weight: Weight for each tree's contribution (learning rate).\n        gating: Gating type or custom GatingFn instance.\n            - \"linear\": LinearGating\n            - \"mlp\": MLPGating with hidden_dim=32\n            - \"tree\": TreeGating (depth auto-computed from num_experts)\n        gating_temperature: Temperature for gating softmax.\n        top_k: If set, only top-k experts are activated (sparse routing).\n        load_balance_weight: Weight for load balancing auxiliary loss.\n        task: \"regression\" or \"classification\".\n    \"\"\"\n    self.num_experts = num_experts\n    self.trees_per_expert = trees_per_expert\n    self.tree_depth = tree_depth\n    self.tree_weight = tree_weight\n    self.gating_temperature = gating_temperature\n    self.top_k = top_k\n    self.load_balance_weight = load_balance_weight\n    self.task = task\n\n    # Initialize gating\n    if isinstance(gating, str):\n        if gating == \"linear\":\n            self.gating = LinearGating()\n        elif gating == \"mlp\":\n            self.gating = MLPGating(hidden_dim=32)\n        elif gating == \"tree\":\n            # Compute depth from num_experts\n            tree_gating_depth = int(jnp.ceil(jnp.log2(num_experts)))\n            if 2 ** tree_gating_depth != num_experts:\n                raise ValueError(\n                    f\"TreeGating requires num_experts to be a power of 2, \"\n                    f\"got {num_experts}. Use {2 ** tree_gating_depth} experts \"\n                    f\"or choose a different gating type.\"\n                )\n            self.gating = TreeGating(depth=tree_gating_depth)\n        else:\n            raise ValueError(f\"Unknown gating type: {gating}\")\n    else:\n        self.gating = gating\n\n    # Tree components (shared across all experts)\n    self.split_fn = HyperplaneSplit()\n    self.tree = ObliviousTree()\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble.fit","title":"fit","text":"<pre><code>fit(\n    X: Array,\n    y: Array,\n    X_val: Array | None = None,\n    y_val: Array | None = None,\n    epochs: int = 300,\n    learning_rate: float = 0.01,\n    temp_start: float = 1.0,\n    temp_end: float = 5.0,\n    patience: int = 50,\n    verbose: bool = True,\n) -&gt; MOEParams\n</code></pre> <p>Train the MOE ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Array</code> <p>Training features, shape (n_samples, n_features).</p> required <code>y</code> <code>Array</code> <p>Training targets, shape (n_samples,).</p> required <code>X_val</code> <code>Array | None</code> <p>Validation features (optional).</p> <code>None</code> <code>y_val</code> <code>Array | None</code> <p>Validation targets (optional).</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>300</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimizer.</p> <code>0.01</code> <code>temp_start</code> <code>float</code> <p>Starting temperature for routing.</p> <code>1.0</code> <code>temp_end</code> <code>float</code> <p>Ending temperature for routing.</p> <code>5.0</code> <code>patience</code> <code>int</code> <p>Early stopping patience.</p> <code>50</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress.</p> <code>True</code> <p>Returns:</p> Type Description <code>MOEParams</code> <p>Trained parameters.</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def fit(\n    self,\n    X: Array,\n    y: Array,\n    X_val: Array | None = None,\n    y_val: Array | None = None,\n    epochs: int = 300,\n    learning_rate: float = 0.01,\n    temp_start: float = 1.0,\n    temp_end: float = 5.0,\n    patience: int = 50,\n    verbose: bool = True,\n) -&gt; MOEParams:\n    \"\"\"Train the MOE ensemble.\n\n    Args:\n        X: Training features, shape (n_samples, n_features).\n        y: Training targets, shape (n_samples,).\n        X_val: Validation features (optional).\n        y_val: Validation targets (optional).\n        epochs: Number of training epochs.\n        learning_rate: Learning rate for optimizer.\n        temp_start: Starting temperature for routing.\n        temp_end: Ending temperature for routing.\n        patience: Early stopping patience.\n        verbose: Whether to print progress.\n\n    Returns:\n        Trained parameters.\n    \"\"\"\n    X = jnp.array(X, dtype=jnp.float32)\n    y = jnp.array(y, dtype=jnp.float32)\n\n    # Create validation split if not provided\n    if X_val is None:\n        n_samples = X.shape[0]\n        split_idx = int(n_samples * 0.85)\n        indices = jax.random.permutation(jax.random.PRNGKey(42), n_samples)\n        train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n    else:\n        X_train, y_train = X, y\n        X_val = jnp.array(X_val, dtype=jnp.float32)\n        y_val = jnp.array(y_val, dtype=jnp.float32)\n\n    # Initialize\n    key = jax.random.PRNGKey(42)\n    num_features = X_train.shape[1]\n    params = self.init_params(key, num_features)\n\n    # Setup optimizer\n    schedule = optax.cosine_decay_schedule(learning_rate, epochs, alpha=0.01)\n    optimizer = optax.adam(schedule)\n    opt_state = optimizer.init(params)\n\n    # Training loop\n    best_val_loss = float('inf')\n    best_params = params\n    no_improve = 0\n\n    @jax.jit\n    def train_step(params, opt_state, temperature):\n        loss_val, grads = jax.value_and_grad(self.loss)(\n            params, X_train, y_train, temperature\n        )\n        updates, opt_state = optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    for epoch in range(epochs):\n        # Temperature annealing\n        progress = epoch / epochs\n        temperature = temp_start + progress * (temp_end - temp_start)\n\n        # Train step\n        params, opt_state, train_loss = train_step(params, opt_state, temperature)\n\n        # Validation\n        if epoch % 10 == 0:\n            val_loss = self.loss(params, X_val, y_val, temperature)\n\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_params = params\n                no_improve = 0\n            else:\n                no_improve += 10\n\n            if verbose and epoch % 50 == 0:\n                # Compute metrics\n                pred_train = self.forward(params, X_train, temperature)\n                pred_val = self.forward(params, X_val, temperature)\n\n                if self.task == \"regression\":\n                    train_metric = jnp.mean((pred_train - y_train) ** 2)\n                    val_metric = jnp.mean((pred_val - y_val) ** 2)\n                    metric_name = \"MSE\"\n                else:\n                    train_metric = jnp.mean((pred_train &gt; 0) == y_train)\n                    val_metric = jnp.mean((pred_val &gt; 0) == y_val)\n                    metric_name = \"Acc\"\n\n                print(\n                    f\"Epoch {epoch}: loss={train_loss:.4f}, \"\n                    f\"train_{metric_name}={train_metric:.4f}, \"\n                    f\"val_{metric_name}={val_metric:.4f}\"\n                )\n\n            if no_improve &gt;= patience:\n                if verbose:\n                    print(f\"Early stopping at epoch {epoch}\")\n                break\n\n    return best_params\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble.predict","title":"predict","text":"<pre><code>predict(\n    params: MOEParams,\n    X: Array,\n    routing_temperature: float = 5.0,\n) -&gt; Array\n</code></pre> <p>Make predictions.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MOEParams</code> <p>Trained parameters.</p> required <code>X</code> <code>Array</code> <p>Features, shape (n_samples, n_features).</p> required <code>routing_temperature</code> <code>float</code> <p>Temperature for routing (use trained temp).</p> <code>5.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Predictions. For regression: values. For classification: probabilities.</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def predict(\n    self,\n    params: MOEParams,\n    X: Array,\n    routing_temperature: float = 5.0,\n) -&gt; Array:\n    \"\"\"Make predictions.\n\n    Args:\n        params: Trained parameters.\n        X: Features, shape (n_samples, n_features).\n        routing_temperature: Temperature for routing (use trained temp).\n\n    Returns:\n        Predictions. For regression: values. For classification: probabilities.\n    \"\"\"\n    X = jnp.array(X, dtype=jnp.float32)\n    output = self.forward(params, X, routing_temperature)\n\n    if self.task == \"classification\":\n        return jax.nn.sigmoid(output)\n    return output\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEEnsemble.get_expert_weights","title":"get_expert_weights","text":"<pre><code>get_expert_weights(params: MOEParams, X: Array) -&gt; Array\n</code></pre> <p>Get expert weights for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MOEParams</code> <p>MOE parameters.</p> required <code>X</code> <code>Array</code> <p>Input features, shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Expert weights, shape (n_samples, num_experts).</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def get_expert_weights(\n    self,\n    params: MOEParams,\n    X: Array,\n) -&gt; Array:\n    \"\"\"Get expert weights for analysis.\n\n    Args:\n        params: MOE parameters.\n        X: Input features, shape (n_samples, n_features).\n\n    Returns:\n        Expert weights, shape (n_samples, num_experts).\n    \"\"\"\n    X = jnp.array(X, dtype=jnp.float32)\n    gate_weights = self.gating(\n        params.gating_params, X, self.gating_temperature\n    )\n    if self.top_k is not None:\n        gate_weights, _ = sparse_top_k(gate_weights, self.top_k)\n    return gate_weights\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TaskAwareMOEEnsemble","title":"TaskAwareMOEEnsemble","text":"<p>               Bases: <code>MOEEnsemble</code></p> <p>MOE with task-aware gating for multi-task learning.</p> <p>Extends MOEEnsemble to accept task IDs, allowing the gating network to route based on both input features and task identity.</p> Example <p>moe = TaskAwareMOEEnsemble(num_experts=4, num_tasks=3) params = moe.init_params(key, num_features=10)</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>class TaskAwareMOEEnsemble(MOEEnsemble):\n    \"\"\"MOE with task-aware gating for multi-task learning.\n\n    Extends MOEEnsemble to accept task IDs, allowing the gating network\n    to route based on both input features and task identity.\n\n    Example:\n        &gt;&gt;&gt; moe = TaskAwareMOEEnsemble(num_experts=4, num_tasks=3)\n        &gt;&gt;&gt; params = moe.init_params(key, num_features=10)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Forward with task ID\n        &gt;&gt;&gt; predictions = moe.forward(params, X, task_ids=task_ids)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_experts: int = 4,\n        num_tasks: int = 1,\n        task_embed_dim: int = 8,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize task-aware MOE.\n\n        Args:\n            num_experts: Number of experts.\n            num_tasks: Number of tasks.\n            task_embed_dim: Dimension of task embeddings.\n            **kwargs: Additional arguments for MOEEnsemble.\n        \"\"\"\n        # Force linear gating for task-aware version\n        kwargs[\"gating\"] = \"linear\"  # Will be overridden\n        super().__init__(num_experts=num_experts, **kwargs)\n\n        self.num_tasks = num_tasks\n        self.task_embed_dim = task_embed_dim\n\n        # Override gating with task-aware version\n        self.gating = _TaskAwareLinearGating(\n            num_tasks=num_tasks,\n            task_embed_dim=task_embed_dim,\n        )\n\n    def forward(\n        self,\n        params: MOEParams,\n        x: Array,\n        task_ids: Array | None = None,\n        routing_temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Forward pass with task awareness.\n\n        Args:\n            params: MOE parameters.\n            x: Input features, shape (batch, num_features).\n            task_ids: Task IDs, shape (batch,). If None, uses task 0.\n            routing_temperature: Temperature for routing.\n\n        Returns:\n            Predictions, shape (batch,).\n        \"\"\"\n        if task_ids is None:\n            task_ids = jnp.zeros(x.shape[0], dtype=jnp.int32)\n\n        single_sample = x.ndim == 1\n        if single_sample:\n            x = x[None, :]\n            task_ids = jnp.array([task_ids])\n\n        # Compute gating weights with task info\n        gate_weights = self.gating(\n            params.gating_params, x, task_ids, self.gating_temperature\n        )\n\n        if self.top_k is not None:\n            gate_weights, _ = sparse_top_k(gate_weights, self.top_k)\n\n        expert_preds = self._compute_expert_predictions(\n            params.expert_params, x, routing_temperature\n        )\n\n        output = jnp.sum(gate_weights * expert_preds, axis=-1)\n\n        return output[0] if single_sample else output\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TaskAwareMOEEnsemble--forward-with-task-id","title":"Forward with task ID","text":"<p>predictions = moe.forward(params, X, task_ids=task_ids)</p>"},{"location":"api/ensemble/#jaxboost.ensemble.TaskAwareMOEEnsemble.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_experts: int = 4,\n    num_tasks: int = 1,\n    task_embed_dim: int = 8,\n    **kwargs,\n) -&gt; None\n</code></pre> <p>Initialize task-aware MOE.</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of experts.</p> <code>4</code> <code>num_tasks</code> <code>int</code> <p>Number of tasks.</p> <code>1</code> <code>task_embed_dim</code> <code>int</code> <p>Dimension of task embeddings.</p> <code>8</code> <code>**kwargs</code> <p>Additional arguments for MOEEnsemble.</p> <code>{}</code> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int = 4,\n    num_tasks: int = 1,\n    task_embed_dim: int = 8,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize task-aware MOE.\n\n    Args:\n        num_experts: Number of experts.\n        num_tasks: Number of tasks.\n        task_embed_dim: Dimension of task embeddings.\n        **kwargs: Additional arguments for MOEEnsemble.\n    \"\"\"\n    # Force linear gating for task-aware version\n    kwargs[\"gating\"] = \"linear\"  # Will be overridden\n    super().__init__(num_experts=num_experts, **kwargs)\n\n    self.num_tasks = num_tasks\n    self.task_embed_dim = task_embed_dim\n\n    # Override gating with task-aware version\n    self.gating = _TaskAwareLinearGating(\n        num_tasks=num_tasks,\n        task_embed_dim=task_embed_dim,\n    )\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TaskAwareMOEEnsemble.forward","title":"forward","text":"<pre><code>forward(\n    params: MOEParams,\n    x: Array,\n    task_ids: Array | None = None,\n    routing_temperature: float = 1.0,\n) -&gt; Array\n</code></pre> <p>Forward pass with task awareness.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MOEParams</code> <p>MOE parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features).</p> required <code>task_ids</code> <code>Array | None</code> <p>Task IDs, shape (batch,). If None, uses task 0.</p> <code>None</code> <code>routing_temperature</code> <code>float</code> <p>Temperature for routing.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Predictions, shape (batch,).</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>def forward(\n    self,\n    params: MOEParams,\n    x: Array,\n    task_ids: Array | None = None,\n    routing_temperature: float = 1.0,\n) -&gt; Array:\n    \"\"\"Forward pass with task awareness.\n\n    Args:\n        params: MOE parameters.\n        x: Input features, shape (batch, num_features).\n        task_ids: Task IDs, shape (batch,). If None, uses task 0.\n        routing_temperature: Temperature for routing.\n\n    Returns:\n        Predictions, shape (batch,).\n    \"\"\"\n    if task_ids is None:\n        task_ids = jnp.zeros(x.shape[0], dtype=jnp.int32)\n\n    single_sample = x.ndim == 1\n    if single_sample:\n        x = x[None, :]\n        task_ids = jnp.array([task_ids])\n\n    # Compute gating weights with task info\n    gate_weights = self.gating(\n        params.gating_params, x, task_ids, self.gating_temperature\n    )\n\n    if self.top_k is not None:\n        gate_weights, _ = sparse_top_k(gate_weights, self.top_k)\n\n    expert_preds = self._compute_expert_predictions(\n        params.expert_params, x, routing_temperature\n    )\n\n    output = jnp.sum(gate_weights * expert_preds, axis=-1)\n\n    return output[0] if single_sample else output\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MOEParams","title":"MOEParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for MOE ensemble.</p> <p>Attributes:</p> Name Type Description <code>gating_params</code> <code>Any</code> <p>Parameters for the gating network.</p> <code>expert_params</code> <code>list[list[Any]]</code> <p>List of expert parameters. Each expert has a list of tree parameters (one per tree in the expert).</p> Source code in <code>src/jaxboost/ensemble/moe.py</code> <pre><code>class MOEParams(NamedTuple):\n    \"\"\"Parameters for MOE ensemble.\n\n    Attributes:\n        gating_params: Parameters for the gating network.\n        expert_params: List of expert parameters. Each expert has a list\n            of tree parameters (one per tree in the expert).\n    \"\"\"\n    gating_params: Any\n    expert_params: list[list[Any]]  # [expert_idx][tree_idx]\n</code></pre>"},{"location":"api/ensemble/#gating-networks","title":"Gating Networks","text":""},{"location":"api/ensemble/#jaxboost.ensemble.GatingFn","title":"GatingFn","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for gating functions.</p> <p>A gating function computes expert weights for each input sample.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>@runtime_checkable\nclass GatingFn(Protocol):\n    \"\"\"Protocol for gating functions.\n\n    A gating function computes expert weights for each input sample.\n    \"\"\"\n\n    def init_params(self, key: Array, num_features: int, num_experts: int) -&gt; Any:\n        \"\"\"Initialize gating parameters.\n\n        Args:\n            key: JAX PRNG key.\n            num_features: Number of input features.\n            num_experts: Number of experts.\n\n        Returns:\n            Gating parameters (any PyTree).\n        \"\"\"\n        ...\n\n    def __call__(\n        self, \n        params: Any, \n        x: Array, \n        temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Compute expert weights.\n\n        Args:\n            params: Gating parameters.\n            x: Input features, shape (batch, num_features).\n            temperature: Softmax temperature. Higher = softer routing.\n\n        Returns:\n            Expert weights, shape (batch, num_experts), softmax normalized.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.GatingFn.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array, num_features: int, num_experts: int\n) -&gt; Any\n</code></pre> <p>Initialize gating parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Gating parameters (any PyTree).</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def init_params(self, key: Array, num_features: int, num_experts: int) -&gt; Any:\n    \"\"\"Initialize gating parameters.\n\n    Args:\n        key: JAX PRNG key.\n        num_features: Number of input features.\n        num_experts: Number of experts.\n\n    Returns:\n        Gating parameters (any PyTree).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.GatingFn.__call__","title":"__call__","text":"<pre><code>__call__(\n    params: Any, x: Array, temperature: float = 1.0\n) -&gt; Array\n</code></pre> <p>Compute expert weights.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Any</code> <p>Gating parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features).</p> required <code>temperature</code> <code>float</code> <p>Softmax temperature. Higher = softer routing.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Expert weights, shape (batch, num_experts), softmax normalized.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __call__(\n    self, \n    params: Any, \n    x: Array, \n    temperature: float = 1.0,\n) -&gt; Array:\n    \"\"\"Compute expert weights.\n\n    Args:\n        params: Gating parameters.\n        x: Input features, shape (batch, num_features).\n        temperature: Softmax temperature. Higher = softer routing.\n\n    Returns:\n        Expert weights, shape (batch, num_experts), softmax normalized.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.LinearGating","title":"LinearGating","text":"<p>Linear gating: g(x) = softmax(W @ x + b).</p> <p>Simple and interpretable. Each row of W represents an expert's \"preference\" for different features.</p> Example <p>gating = LinearGating() params = gating.init_params(key, num_features=10, num_experts=4) weights = gating(params, x)  # (batch, 4)</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>class LinearGating:\n    \"\"\"Linear gating: g(x) = softmax(W @ x + b).\n\n    Simple and interpretable. Each row of W represents an expert's\n    \"preference\" for different features.\n\n    Example:\n        &gt;&gt;&gt; gating = LinearGating()\n        &gt;&gt;&gt; params = gating.init_params(key, num_features=10, num_experts=4)\n        &gt;&gt;&gt; weights = gating(params, x)  # (batch, 4)\n    \"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        num_experts: int,\n        init_scale: float = 0.01,\n    ) -&gt; LinearGatingParams:\n        \"\"\"Initialize gating parameters.\n\n        Args:\n            key: JAX PRNG key.\n            num_features: Number of input features.\n            num_experts: Number of experts.\n            init_scale: Scale for weight initialization.\n\n        Returns:\n            Initialized parameters.\n        \"\"\"\n        W = jax.random.normal(key, (num_experts, num_features)) * init_scale\n        b = jnp.zeros(num_experts)\n        return LinearGatingParams(W=W, b=b)\n\n    def __call__(\n        self,\n        params: LinearGatingParams,\n        x: Array,\n        temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Compute expert weights.\n\n        Args:\n            params: Gating parameters.\n            x: Input features, shape (batch, num_features).\n            temperature: Softmax temperature.\n\n        Returns:\n            Expert weights, shape (batch, num_experts).\n        \"\"\"\n        # x: (batch, num_features), W: (num_experts, num_features)\n        logits = x @ params.W.T + params.b  # (batch, num_experts)\n        return jax.nn.softmax(logits / temperature, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.LinearGating.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    num_features: int,\n    num_experts: int,\n    init_scale: float = 0.01,\n) -&gt; LinearGatingParams\n</code></pre> <p>Initialize gating parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts.</p> required <code>init_scale</code> <code>float</code> <p>Scale for weight initialization.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>LinearGatingParams</code> <p>Initialized parameters.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    num_experts: int,\n    init_scale: float = 0.01,\n) -&gt; LinearGatingParams:\n    \"\"\"Initialize gating parameters.\n\n    Args:\n        key: JAX PRNG key.\n        num_features: Number of input features.\n        num_experts: Number of experts.\n        init_scale: Scale for weight initialization.\n\n    Returns:\n        Initialized parameters.\n    \"\"\"\n    W = jax.random.normal(key, (num_experts, num_features)) * init_scale\n    b = jnp.zeros(num_experts)\n    return LinearGatingParams(W=W, b=b)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.LinearGating.__call__","title":"__call__","text":"<pre><code>__call__(\n    params: LinearGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array\n</code></pre> <p>Compute expert weights.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>LinearGatingParams</code> <p>Gating parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features).</p> required <code>temperature</code> <code>float</code> <p>Softmax temperature.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Expert weights, shape (batch, num_experts).</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __call__(\n    self,\n    params: LinearGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array:\n    \"\"\"Compute expert weights.\n\n    Args:\n        params: Gating parameters.\n        x: Input features, shape (batch, num_features).\n        temperature: Softmax temperature.\n\n    Returns:\n        Expert weights, shape (batch, num_experts).\n    \"\"\"\n    # x: (batch, num_features), W: (num_experts, num_features)\n    logits = x @ params.W.T + params.b  # (batch, num_experts)\n    return jax.nn.softmax(logits / temperature, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MLPGating","title":"MLPGating","text":"<p>MLP gating: g(x) = softmax(W2 @ relu(W1 @ x + b1) + b2).</p> <p>Two-layer MLP for non-linear routing decisions. More expressive than linear, but may overfit on small data.</p> Example <p>gating = MLPGating(hidden_dim=32) params = gating.init_params(key, num_features=10, num_experts=4) weights = gating(params, x)  # (batch, 4)</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>class MLPGating:\n    \"\"\"MLP gating: g(x) = softmax(W2 @ relu(W1 @ x + b1) + b2).\n\n    Two-layer MLP for non-linear routing decisions.\n    More expressive than linear, but may overfit on small data.\n\n    Example:\n        &gt;&gt;&gt; gating = MLPGating(hidden_dim=32)\n        &gt;&gt;&gt; params = gating.init_params(key, num_features=10, num_experts=4)\n        &gt;&gt;&gt; weights = gating(params, x)  # (batch, 4)\n    \"\"\"\n\n    def __init__(self, hidden_dim: int = 32) -&gt; None:\n        \"\"\"Initialize MLP gating.\n\n        Args:\n            hidden_dim: Hidden layer dimension.\n        \"\"\"\n        self.hidden_dim = hidden_dim\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        num_experts: int,\n        init_scale: float = 0.01,\n    ) -&gt; MLPGatingParams:\n        \"\"\"Initialize gating parameters.\n\n        Args:\n            key: JAX PRNG key.\n            num_features: Number of input features.\n            num_experts: Number of experts.\n            init_scale: Scale for weight initialization.\n\n        Returns:\n            Initialized parameters.\n        \"\"\"\n        k1, k2 = jax.random.split(key)\n\n        # Xavier-style initialization\n        std1 = init_scale * jnp.sqrt(2.0 / (num_features + self.hidden_dim))\n        std2 = init_scale * jnp.sqrt(2.0 / (self.hidden_dim + num_experts))\n\n        W1 = jax.random.normal(k1, (self.hidden_dim, num_features)) * std1\n        b1 = jnp.zeros(self.hidden_dim)\n        W2 = jax.random.normal(k2, (num_experts, self.hidden_dim)) * std2\n        b2 = jnp.zeros(num_experts)\n\n        return MLPGatingParams(W1=W1, b1=b1, W2=W2, b2=b2)\n\n    def __call__(\n        self,\n        params: MLPGatingParams,\n        x: Array,\n        temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Compute expert weights.\n\n        Args:\n            params: Gating parameters.\n            x: Input features, shape (batch, num_features).\n            temperature: Softmax temperature.\n\n        Returns:\n            Expert weights, shape (batch, num_experts).\n        \"\"\"\n        # First layer\n        h = jax.nn.relu(x @ params.W1.T + params.b1)  # (batch, hidden_dim)\n\n        # Second layer\n        logits = h @ params.W2.T + params.b2  # (batch, num_experts)\n\n        return jax.nn.softmax(logits / temperature, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MLPGating.__init__","title":"__init__","text":"<pre><code>__init__(hidden_dim: int = 32) -&gt; None\n</code></pre> <p>Initialize MLP gating.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dim</code> <code>int</code> <p>Hidden layer dimension.</p> <code>32</code> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __init__(self, hidden_dim: int = 32) -&gt; None:\n    \"\"\"Initialize MLP gating.\n\n    Args:\n        hidden_dim: Hidden layer dimension.\n    \"\"\"\n    self.hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MLPGating.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    num_features: int,\n    num_experts: int,\n    init_scale: float = 0.01,\n) -&gt; MLPGatingParams\n</code></pre> <p>Initialize gating parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts.</p> required <code>init_scale</code> <code>float</code> <p>Scale for weight initialization.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>MLPGatingParams</code> <p>Initialized parameters.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    num_experts: int,\n    init_scale: float = 0.01,\n) -&gt; MLPGatingParams:\n    \"\"\"Initialize gating parameters.\n\n    Args:\n        key: JAX PRNG key.\n        num_features: Number of input features.\n        num_experts: Number of experts.\n        init_scale: Scale for weight initialization.\n\n    Returns:\n        Initialized parameters.\n    \"\"\"\n    k1, k2 = jax.random.split(key)\n\n    # Xavier-style initialization\n    std1 = init_scale * jnp.sqrt(2.0 / (num_features + self.hidden_dim))\n    std2 = init_scale * jnp.sqrt(2.0 / (self.hidden_dim + num_experts))\n\n    W1 = jax.random.normal(k1, (self.hidden_dim, num_features)) * std1\n    b1 = jnp.zeros(self.hidden_dim)\n    W2 = jax.random.normal(k2, (num_experts, self.hidden_dim)) * std2\n    b2 = jnp.zeros(num_experts)\n\n    return MLPGatingParams(W1=W1, b1=b1, W2=W2, b2=b2)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.MLPGating.__call__","title":"__call__","text":"<pre><code>__call__(\n    params: MLPGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array\n</code></pre> <p>Compute expert weights.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MLPGatingParams</code> <p>Gating parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features).</p> required <code>temperature</code> <code>float</code> <p>Softmax temperature.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Expert weights, shape (batch, num_experts).</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __call__(\n    self,\n    params: MLPGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array:\n    \"\"\"Compute expert weights.\n\n    Args:\n        params: Gating parameters.\n        x: Input features, shape (batch, num_features).\n        temperature: Softmax temperature.\n\n    Returns:\n        Expert weights, shape (batch, num_experts).\n    \"\"\"\n    # First layer\n    h = jax.nn.relu(x @ params.W1.T + params.b1)  # (batch, hidden_dim)\n\n    # Second layer\n    logits = h @ params.W2.T + params.b2  # (batch, num_experts)\n\n    return jax.nn.softmax(logits / temperature, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TreeGating","title":"TreeGating","text":"<p>Tree-based gating using soft decision tree.</p> <p>Uses an oblivious soft decision tree where leaf probabilities become expert weights. Most interpretable option.</p> <p>The number of experts is determined by tree depth: num_experts = 2^depth.</p> Example <p>gating = TreeGating(depth=2)  # 4 experts params = gating.init_params(key, num_features=10) weights = gating(params, x)  # (batch, 4)</p> Interpretability <p>Each path through the tree defines an expert's \"activation region\". - Expert 0: left at depth 0, left at depth 1 - Expert 1: left at depth 0, right at depth 1 - Expert 2: right at depth 0, left at depth 1 - Expert 3: right at depth 0, right at depth 1</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>class TreeGating:\n    \"\"\"Tree-based gating using soft decision tree.\n\n    Uses an oblivious soft decision tree where leaf probabilities\n    become expert weights. Most interpretable option.\n\n    The number of experts is determined by tree depth: num_experts = 2^depth.\n\n    Example:\n        &gt;&gt;&gt; gating = TreeGating(depth=2)  # 4 experts\n        &gt;&gt;&gt; params = gating.init_params(key, num_features=10)\n        &gt;&gt;&gt; weights = gating(params, x)  # (batch, 4)\n\n    Interpretability:\n        Each path through the tree defines an expert's \"activation region\".\n        - Expert 0: left at depth 0, left at depth 1\n        - Expert 1: left at depth 0, right at depth 1\n        - Expert 2: right at depth 0, left at depth 1\n        - Expert 3: right at depth 0, right at depth 1\n    \"\"\"\n\n    def __init__(self, depth: int = 2) -&gt; None:\n        \"\"\"Initialize tree gating.\n\n        Args:\n            depth: Tree depth. Number of experts = 2^depth.\n        \"\"\"\n        self.depth = depth\n        self.num_experts = 2 ** depth\n        self.split_fn = HyperplaneSplit()\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        num_experts: int | None = None,\n    ) -&gt; TreeGatingParams:\n        \"\"\"Initialize gating parameters.\n\n        Args:\n            key: JAX PRNG key.\n            num_features: Number of input features.\n            num_experts: Ignored (determined by depth). Kept for API consistency.\n\n        Returns:\n            Initialized parameters.\n        \"\"\"\n        if num_experts is not None and num_experts != self.num_experts:\n            raise ValueError(\n                f\"TreeGating with depth={self.depth} has {self.num_experts} experts, \"\n                f\"but num_experts={num_experts} was requested. \"\n                f\"Use depth={int(jnp.log2(num_experts))} for {num_experts} experts.\"\n            )\n\n        keys = jax.random.split(key, self.depth)\n        split_params = [\n            self.split_fn.init_params(keys[d], num_features)\n            for d in range(self.depth)\n        ]\n\n        return TreeGatingParams(split_params=split_params)\n\n    def __call__(\n        self,\n        params: TreeGatingParams,\n        x: Array,\n        temperature: float = 1.0,\n    ) -&gt; Array:\n        \"\"\"Compute expert weights (leaf probabilities).\n\n        Args:\n            params: Gating parameters.\n            x: Input features, shape (batch, num_features).\n            temperature: Routing temperature.\n\n        Returns:\n            Expert weights, shape (batch, num_experts).\n        \"\"\"\n        single_sample = x.ndim == 1\n        if single_sample:\n            x = x[None, :]\n\n        # Compute routing probabilities at each depth\n        p_rights = []\n        for d in range(self.depth):\n            score = self.split_fn.compute_score(params.split_params[d], x)\n            p_right = soft_routing(score, temperature)\n            p_rights.append(p_right)\n        p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n        # Compute leaf probabilities (expert weights)\n        leaf_probs = self._compute_leaf_probs(p_rights)  # (batch, num_experts)\n\n        if single_sample:\n            return leaf_probs[0]\n        return leaf_probs\n\n    def _compute_leaf_probs(self, p_rights: Array) -&gt; Array:\n        \"\"\"Compute probability of reaching each leaf (expert).\n\n        Uses bit manipulation + broadcasting for efficiency.\n\n        Args:\n            p_rights: Right routing probabilities, shape (depth, batch).\n\n        Returns:\n            Leaf probabilities, shape (batch, num_experts).\n        \"\"\"\n        # Create binary mask: which way to go at each depth for each leaf\n        leaf_indices = jnp.arange(self.num_experts)\n        depth_indices = jnp.arange(self.depth)\n\n        # bit_mask[leaf, depth] = 1 if leaf goes right at depth, else 0\n        bit_mask = (leaf_indices[:, None] &gt;&gt; depth_indices) &amp; 1  # (num_experts, depth)\n\n        # p_rights: (depth, batch) -&gt; (batch, depth)\n        p_rights_T = p_rights.T  # (batch, depth)\n\n        # Broadcast: (batch, 1, depth) vs (1, num_experts, depth)\n        p_rights_expanded = p_rights_T[:, None, :]  # (batch, 1, depth)\n        bit_mask_expanded = bit_mask[None, :, :]  # (1, num_experts, depth)\n\n        # routing_prob = p_right if go_right else (1 - p_right)\n        routing_probs = (\n            bit_mask_expanded * p_rights_expanded\n            + (1 - bit_mask_expanded) * (1 - p_rights_expanded)\n        )  # (batch, num_experts, depth)\n\n        # Product over depth dimension\n        leaf_probs = jnp.prod(routing_probs, axis=-1)  # (batch, num_experts)\n\n        return leaf_probs\n\n    def get_routing_rules(self, params: TreeGatingParams) -&gt; list[str]:\n        \"\"\"Get human-readable routing rules for each expert.\n\n        Useful for interpretability.\n\n        Args:\n            params: Gating parameters.\n\n        Returns:\n            List of rule strings, one per expert.\n        \"\"\"\n        rules = []\n        for expert_idx in range(self.num_experts):\n            conditions = []\n            for d in range(self.depth):\n                go_right = (expert_idx &gt;&gt; d) &amp; 1\n                direction = \"&gt;\" if go_right else \"&lt;=\"\n                conditions.append(f\"split_{d} {direction} 0\")\n            rules.append(f\"Expert {expert_idx}: \" + \" AND \".join(conditions))\n        return rules\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TreeGating.__init__","title":"__init__","text":"<pre><code>__init__(depth: int = 2) -&gt; None\n</code></pre> <p>Initialize tree gating.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>Tree depth. Number of experts = 2^depth.</p> <code>2</code> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __init__(self, depth: int = 2) -&gt; None:\n    \"\"\"Initialize tree gating.\n\n    Args:\n        depth: Tree depth. Number of experts = 2^depth.\n    \"\"\"\n    self.depth = depth\n    self.num_experts = 2 ** depth\n    self.split_fn = HyperplaneSplit()\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TreeGating.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    num_features: int,\n    num_experts: int | None = None,\n) -&gt; TreeGatingParams\n</code></pre> <p>Initialize gating parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>num_experts</code> <code>int | None</code> <p>Ignored (determined by depth). Kept for API consistency.</p> <code>None</code> <p>Returns:</p> Type Description <code>TreeGatingParams</code> <p>Initialized parameters.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    num_experts: int | None = None,\n) -&gt; TreeGatingParams:\n    \"\"\"Initialize gating parameters.\n\n    Args:\n        key: JAX PRNG key.\n        num_features: Number of input features.\n        num_experts: Ignored (determined by depth). Kept for API consistency.\n\n    Returns:\n        Initialized parameters.\n    \"\"\"\n    if num_experts is not None and num_experts != self.num_experts:\n        raise ValueError(\n            f\"TreeGating with depth={self.depth} has {self.num_experts} experts, \"\n            f\"but num_experts={num_experts} was requested. \"\n            f\"Use depth={int(jnp.log2(num_experts))} for {num_experts} experts.\"\n        )\n\n    keys = jax.random.split(key, self.depth)\n    split_params = [\n        self.split_fn.init_params(keys[d], num_features)\n        for d in range(self.depth)\n    ]\n\n    return TreeGatingParams(split_params=split_params)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TreeGating.__call__","title":"__call__","text":"<pre><code>__call__(\n    params: TreeGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array\n</code></pre> <p>Compute expert weights (leaf probabilities).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TreeGatingParams</code> <p>Gating parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features).</p> required <code>temperature</code> <code>float</code> <p>Routing temperature.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Expert weights, shape (batch, num_experts).</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def __call__(\n    self,\n    params: TreeGatingParams,\n    x: Array,\n    temperature: float = 1.0,\n) -&gt; Array:\n    \"\"\"Compute expert weights (leaf probabilities).\n\n    Args:\n        params: Gating parameters.\n        x: Input features, shape (batch, num_features).\n        temperature: Routing temperature.\n\n    Returns:\n        Expert weights, shape (batch, num_experts).\n    \"\"\"\n    single_sample = x.ndim == 1\n    if single_sample:\n        x = x[None, :]\n\n    # Compute routing probabilities at each depth\n    p_rights = []\n    for d in range(self.depth):\n        score = self.split_fn.compute_score(params.split_params[d], x)\n        p_right = soft_routing(score, temperature)\n        p_rights.append(p_right)\n    p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n    # Compute leaf probabilities (expert weights)\n    leaf_probs = self._compute_leaf_probs(p_rights)  # (batch, num_experts)\n\n    if single_sample:\n        return leaf_probs[0]\n    return leaf_probs\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.TreeGating.get_routing_rules","title":"get_routing_rules","text":"<pre><code>get_routing_rules(params: TreeGatingParams) -&gt; list[str]\n</code></pre> <p>Get human-readable routing rules for each expert.</p> <p>Useful for interpretability.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TreeGatingParams</code> <p>Gating parameters.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of rule strings, one per expert.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def get_routing_rules(self, params: TreeGatingParams) -&gt; list[str]:\n    \"\"\"Get human-readable routing rules for each expert.\n\n    Useful for interpretability.\n\n    Args:\n        params: Gating parameters.\n\n    Returns:\n        List of rule strings, one per expert.\n    \"\"\"\n    rules = []\n    for expert_idx in range(self.num_experts):\n        conditions = []\n        for d in range(self.depth):\n            go_right = (expert_idx &gt;&gt; d) &amp; 1\n            direction = \"&gt;\" if go_right else \"&lt;=\"\n            conditions.append(f\"split_{d} {direction} 0\")\n        rules.append(f\"Expert {expert_idx}: \" + \" AND \".join(conditions))\n    return rules\n</code></pre>"},{"location":"api/ensemble/#em-moe","title":"EM-MOE","text":"<p>External GBDT experts (XGBoost, LightGBM, CatBoost) trained via EM algorithm.</p>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE","title":"EMMOE","text":"<p>EM-trained Mixture of Experts with external GBDT experts.</p> <p>This implementation uses proper EM algorithm:</p> <p>Model: p(y|x) = \u03a3_k g_k(x) \u00b7 p(y|x, expert_k)</p> <p>E-step: Compute responsibilities (posterior over experts):     \u03b3_{ik} = g_k(x_i) \u00b7 N(y_i; f_k(x_i), \u03c3\u00b2) / \u03a3_j g_j(x_i) \u00b7 N(y_i; f_j(x_i), \u03c3\u00b2)</p> <p>M-step:      - Train expert k on all data with sample_weight = \u03b3_{:,k}     - Train gating to minimize KL(\u03b3 || g(x)) or maximize responsibility prediction</p> Example <p>experts = [ ...     create_xgboost_expert(task=\"regression\", n_estimators=100) ...     for _ in range(4) ... ] moe = EMMOE(experts, config=EMConfig(em_iterations=10)) moe.fit(X_train, y_train) y_pred = moe.predict(X_test)</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>class EMMOE:\n    \"\"\"EM-trained Mixture of Experts with external GBDT experts.\n\n    This implementation uses proper EM algorithm:\n\n    **Model**: p(y|x) = \u03a3_k g_k(x) \u00b7 p(y|x, expert_k)\n\n    **E-step**: Compute responsibilities (posterior over experts):\n        \u03b3_{ik} = g_k(x_i) \u00b7 N(y_i; f_k(x_i), \u03c3\u00b2) / \u03a3_j g_j(x_i) \u00b7 N(y_i; f_j(x_i), \u03c3\u00b2)\n\n    **M-step**: \n        - Train expert k on all data with sample_weight = \u03b3_{:,k}\n        - Train gating to minimize KL(\u03b3 || g(x)) or maximize responsibility prediction\n\n    Example:\n        &gt;&gt;&gt; experts = [\n        ...     create_xgboost_expert(task=\"regression\", n_estimators=100)\n        ...     for _ in range(4)\n        ... ]\n        &gt;&gt;&gt; moe = EMMOE(experts, config=EMConfig(em_iterations=10))\n        &gt;&gt;&gt; moe.fit(X_train, y_train)\n        &gt;&gt;&gt; y_pred = moe.predict(X_test)\n    \"\"\"\n\n    def __init__(\n        self,\n        experts: list[ExternalGBDTExpert],\n        config: EMConfig | None = None,\n        task: Literal[\"regression\", \"classification\"] = \"regression\",\n    ):\n        self.experts = experts\n        self.config = config or EMConfig(num_experts=len(experts))\n        self.task = task\n\n        if len(experts) != self.config.num_experts:\n            raise ValueError(\n                f\"Number of experts ({len(experts)}) must match \"\n                f\"config.num_experts ({self.config.num_experts})\"\n            )\n\n        self.gating = GatingNetwork(\n            hidden_dims=self.config.gating_hidden_dims,\n            temperature=self.config.temperature,\n        )\n        self.gating_params = None\n        self._fitted = False\n        self.history = {\"log_likelihood\": [], \"expert_usage\": []}\n\n    def _init_experts(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"Initialize experts before EM iterations.\"\"\"\n        n_samples = X.shape[0]\n        K = self.config.num_experts\n        strategy = self.config.expert_init_strategy\n\n        if strategy == \"random\":\n            indices = np.random.permutation(n_samples)\n            splits = np.array_split(indices, K)\n            for k, idx in enumerate(splits):\n                if len(idx) &gt; 0:\n                    self.experts[k].fit(X[idx], y[idx])\n\n        elif strategy == \"cluster\":\n            from sklearn.cluster import KMeans\n            kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n            labels = kmeans.fit_predict(X)\n            for k in range(K):\n                mask = labels == k\n                if mask.sum() &gt; 0:\n                    self.experts[k].fit(X[mask], y[mask])\n\n        elif strategy == \"bootstrap\":\n            for k in range(K):\n                rng = np.random.default_rng(42 + k)\n                idx = rng.choice(n_samples, size=n_samples, replace=True)\n                self.experts[k].fit(X[idx], y[idx])\n\n        elif strategy == \"uniform\":\n            # All experts start from same data\n            for expert in self.experts:\n                expert.fit(X, y)\n        else:\n            raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    def _get_expert_predictions(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Get predictions from all experts: (n_samples, num_experts).\"\"\"\n        preds = []\n        for expert in self.experts:\n            try:\n                preds.append(expert.predict(X))\n            except ValueError:\n                preds.append(np.zeros(X.shape[0]))\n        return np.stack(preds, axis=-1)\n\n    def _compute_responsibilities(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        expert_preds: np.ndarray,\n        gating_weights: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"E-step: Compute posterior responsibilities \u03b3_{ik}.\"\"\"\n        sigma2 = self.config.noise_variance\n\n        if self.task == \"regression\":\n            # Gaussian likelihood: p(y|x,k) \u221d exp(-0.5 * (y - f_k(x))\u00b2 / \u03c3\u00b2)\n            errors = y[:, None] - expert_preds  # (n, K)\n            log_likelihood = -0.5 * errors**2 / sigma2  # (n, K)\n        else:\n            # Bernoulli likelihood for classification\n            eps = 1e-7\n            probs = np.clip(expert_preds, eps, 1 - eps)\n            log_likelihood = y[:, None] * np.log(probs) + (1 - y[:, None]) * np.log(1 - probs)\n\n        # Log posterior: log(\u03b3_ik) = log(g_k) + log(p(y|x,k)) - log(\u03a3_j ...)\n        log_weights = np.log(gating_weights + 1e-10)\n        log_joint = log_weights + log_likelihood  # (n, K)\n\n        # Normalize via log-sum-exp for numerical stability\n        log_sum = np.logaddexp.reduce(log_joint, axis=-1, keepdims=True)\n        log_responsibilities = log_joint - log_sum\n        responsibilities = np.exp(log_responsibilities)\n\n        return responsibilities\n\n    def _m_step_experts(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        responsibilities: np.ndarray,\n    ) -&gt; None:\n        \"\"\"M-step for experts: Retrain with responsibility-weighted samples.\"\"\"\n        for k in range(self.config.num_experts):\n            weights = responsibilities[:, k]\n            # Only train if expert has significant total responsibility\n            if weights.sum() &gt; self.config.min_responsibility * len(X):\n                self.experts[k].fit(X, y, sample_weight=weights)\n\n    def _m_step_gating(\n        self,\n        X: np.ndarray,\n        responsibilities: np.ndarray,\n        epochs: int,\n        learning_rate: float,\n    ) -&gt; float:\n        \"\"\"M-step for gating: Train to predict responsibilities.\n\n        Minimize cross-entropy between gating outputs and responsibilities:\n            L = -\u03a3_i \u03a3_k \u03b3_{ik} \u00b7 log(g_k(x_i))\n        \"\"\"\n        X_jax = jnp.array(X, dtype=jnp.float32)\n        resp_jax = jnp.array(responsibilities, dtype=jnp.float32)\n\n        def loss_fn(params):\n            g = self.gating(params, X_jax)\n            # Cross-entropy: -\u03a3 \u03b3_k \u00b7 log(g_k)\n            return -jnp.mean(jnp.sum(resp_jax * jnp.log(g + 1e-10), axis=-1))\n\n        optimizer = optax.adam(learning_rate)\n        opt_state = optimizer.init(self.gating_params)\n\n        @jax.jit\n        def train_step(params, opt_state):\n            loss, grads = jax.value_and_grad(loss_fn)(params)\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss\n\n        final_loss = 0.0\n        for _ in range(epochs):\n            self.gating_params, opt_state, final_loss = train_step(\n                self.gating_params, opt_state\n            )\n\n        return float(final_loss)\n\n    def _compute_log_likelihood(\n        self,\n        y: np.ndarray,\n        expert_preds: np.ndarray,\n        gating_weights: np.ndarray,\n    ) -&gt; float:\n        \"\"\"Compute log-likelihood: log p(y|x) = log \u03a3_k g_k \u00b7 p(y|x,k).\"\"\"\n        sigma2 = self.config.noise_variance\n\n        if self.task == \"regression\":\n            errors = y[:, None] - expert_preds\n            log_likelihood = -0.5 * errors**2 / sigma2 - 0.5 * np.log(2 * np.pi * sigma2)\n        else:\n            eps = 1e-7\n            probs = np.clip(expert_preds, eps, 1 - eps)\n            log_likelihood = y[:, None] * np.log(probs) + (1 - y[:, None]) * np.log(1 - probs)\n\n        # log p(y|x) = log \u03a3_k g_k \u00b7 exp(log_lik_k)\n        log_joint = np.log(gating_weights + 1e-10) + log_likelihood\n        log_marginal = np.logaddexp.reduce(log_joint, axis=-1)\n\n        return float(np.mean(log_marginal))\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        verbose: bool = True,\n    ) -&gt; \"EMMOE\":\n        \"\"\"Train MOE using EM algorithm.\n\n        Args:\n            X: Training features, shape (n_samples, n_features).\n            y: Training targets, shape (n_samples,).\n            verbose: Whether to print progress.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        X = np.asarray(X, dtype=np.float32)\n        y = np.asarray(y, dtype=np.float32)\n\n        # Initialize gating network\n        key = jax.random.PRNGKey(42)\n        self.gating_params = self.gating.init_params(\n            key, X.shape[1], self.config.num_experts\n        )\n\n        # Initialize experts\n        if verbose:\n            print(f\"Initializing {self.config.num_experts} experts \"\n                  f\"(strategy: {self.config.expert_init_strategy})...\")\n        self._init_experts(X, y)\n\n        # EM iterations\n        for em_iter in range(self.config.em_iterations):\n            # Get current predictions\n            expert_preds = self._get_expert_predictions(X)\n            X_jax = jnp.array(X, dtype=jnp.float32)\n            gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n            # Compute log-likelihood before update\n            ll = self._compute_log_likelihood(y, expert_preds, gating_weights)\n            self.history[\"log_likelihood\"].append(ll)\n\n            # E-step: Compute responsibilities\n            responsibilities = self._compute_responsibilities(\n                X, y, expert_preds, gating_weights\n            )\n\n            # Track expert usage\n            expert_usage = responsibilities.mean(axis=0)\n            self.history[\"expert_usage\"].append(expert_usage.tolist())\n\n            if verbose:\n                usage_str = \", \".join(f\"{u:.2f}\" for u in expert_usage)\n                print(f\"EM iter {em_iter + 1}/{self.config.em_iterations}: \"\n                      f\"LL={ll:.4f}, usage=[{usage_str}]\")\n\n            # M-step: Update experts\n            self._m_step_experts(X, y, responsibilities)\n\n            # M-step: Update gating\n            self._m_step_gating(\n                X, responsibilities,\n                epochs=self.config.gating_epochs_per_iter,\n                learning_rate=self.config.gating_lr,\n            )\n\n        # Final log-likelihood\n        expert_preds = self._get_expert_predictions(X)\n        gating_weights = np.array(self.gating(self.gating_params, jnp.array(X, dtype=jnp.float32)))\n        final_ll = self._compute_log_likelihood(y, expert_preds, gating_weights)\n        self.history[\"log_likelihood\"].append(final_ll)\n\n        if verbose:\n            print(f\"Final LL: {final_ll:.4f}\")\n\n        self._fitted = True\n        return self\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Make predictions: weighted sum of expert outputs.\"\"\"\n        if not self._fitted:\n            raise ValueError(\"MOE not fitted. Call fit() first.\")\n\n        X = np.asarray(X, dtype=np.float32)\n        X_jax = jnp.array(X, dtype=jnp.float32)\n\n        expert_preds = self._get_expert_predictions(X)\n        gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n        return np.sum(gating_weights * expert_preds, axis=-1)\n\n    def predict_with_uncertainty(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Predict with epistemic uncertainty from expert disagreement.\n\n        Returns:\n            mean: Weighted mean predictions\n            std: Weighted standard deviation across experts\n        \"\"\"\n        X = np.asarray(X, dtype=np.float32)\n        X_jax = jnp.array(X, dtype=jnp.float32)\n\n        expert_preds = self._get_expert_predictions(X)\n        weights = np.array(self.gating(self.gating_params, X_jax))\n\n        # Weighted mean\n        mean = np.sum(weights * expert_preds, axis=-1)\n\n        # Weighted variance: Var = E[f\u00b2] - E[f]\u00b2\n        mean_sq = np.sum(weights * expert_preds**2, axis=-1)\n        variance = mean_sq - mean**2\n        std = np.sqrt(np.maximum(variance, 0))\n\n        return mean, std\n\n    def get_responsibilities(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Get expert responsibilities for given samples.\"\"\"\n        X = np.asarray(X, dtype=np.float32)\n        y = np.asarray(y, dtype=np.float32)\n\n        expert_preds = self._get_expert_predictions(X)\n        X_jax = jnp.array(X, dtype=jnp.float32)\n        gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n        return self._compute_responsibilities(X, y, expert_preds, gating_weights)\n\n    def get_expert_weights(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Get gating weights (prior over experts) for inputs.\"\"\"\n        X_jax = jnp.array(X, dtype=jnp.float32)\n        return np.array(self.gating(self.gating_params, X_jax))\n\n    def get_expert_predictions(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Get individual expert predictions.\"\"\"\n        return self._get_expert_predictions(np.asarray(X, dtype=np.float32))\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.__init__","title":"__init__","text":"<pre><code>__init__(\n    experts: list[ExternalGBDTExpert],\n    config: EMConfig | None = None,\n    task: Literal[\n        \"regression\", \"classification\"\n    ] = \"regression\",\n)\n</code></pre> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def __init__(\n    self,\n    experts: list[ExternalGBDTExpert],\n    config: EMConfig | None = None,\n    task: Literal[\"regression\", \"classification\"] = \"regression\",\n):\n    self.experts = experts\n    self.config = config or EMConfig(num_experts=len(experts))\n    self.task = task\n\n    if len(experts) != self.config.num_experts:\n        raise ValueError(\n            f\"Number of experts ({len(experts)}) must match \"\n            f\"config.num_experts ({self.config.num_experts})\"\n        )\n\n    self.gating = GatingNetwork(\n        hidden_dims=self.config.gating_hidden_dims,\n        temperature=self.config.temperature,\n    )\n    self.gating_params = None\n    self._fitted = False\n    self.history = {\"log_likelihood\": [], \"expert_usage\": []}\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray, y: ndarray, verbose: bool = True\n) -&gt; \"EMMOE\"\n</code></pre> <p>Train MOE using EM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training features, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> required <code>verbose</code> <code>bool</code> <p>Whether to print progress.</p> <code>True</code> <p>Returns:</p> Type Description <code>'EMMOE'</code> <p>Self for method chaining.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    verbose: bool = True,\n) -&gt; \"EMMOE\":\n    \"\"\"Train MOE using EM algorithm.\n\n    Args:\n        X: Training features, shape (n_samples, n_features).\n        y: Training targets, shape (n_samples,).\n        verbose: Whether to print progress.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    y = np.asarray(y, dtype=np.float32)\n\n    # Initialize gating network\n    key = jax.random.PRNGKey(42)\n    self.gating_params = self.gating.init_params(\n        key, X.shape[1], self.config.num_experts\n    )\n\n    # Initialize experts\n    if verbose:\n        print(f\"Initializing {self.config.num_experts} experts \"\n              f\"(strategy: {self.config.expert_init_strategy})...\")\n    self._init_experts(X, y)\n\n    # EM iterations\n    for em_iter in range(self.config.em_iterations):\n        # Get current predictions\n        expert_preds = self._get_expert_predictions(X)\n        X_jax = jnp.array(X, dtype=jnp.float32)\n        gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n        # Compute log-likelihood before update\n        ll = self._compute_log_likelihood(y, expert_preds, gating_weights)\n        self.history[\"log_likelihood\"].append(ll)\n\n        # E-step: Compute responsibilities\n        responsibilities = self._compute_responsibilities(\n            X, y, expert_preds, gating_weights\n        )\n\n        # Track expert usage\n        expert_usage = responsibilities.mean(axis=0)\n        self.history[\"expert_usage\"].append(expert_usage.tolist())\n\n        if verbose:\n            usage_str = \", \".join(f\"{u:.2f}\" for u in expert_usage)\n            print(f\"EM iter {em_iter + 1}/{self.config.em_iterations}: \"\n                  f\"LL={ll:.4f}, usage=[{usage_str}]\")\n\n        # M-step: Update experts\n        self._m_step_experts(X, y, responsibilities)\n\n        # M-step: Update gating\n        self._m_step_gating(\n            X, responsibilities,\n            epochs=self.config.gating_epochs_per_iter,\n            learning_rate=self.config.gating_lr,\n        )\n\n    # Final log-likelihood\n    expert_preds = self._get_expert_predictions(X)\n    gating_weights = np.array(self.gating(self.gating_params, jnp.array(X, dtype=jnp.float32)))\n    final_ll = self._compute_log_likelihood(y, expert_preds, gating_weights)\n    self.history[\"log_likelihood\"].append(final_ll)\n\n    if verbose:\n        print(f\"Final LL: {final_ll:.4f}\")\n\n    self._fitted = True\n    return self\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Make predictions: weighted sum of expert outputs.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Make predictions: weighted sum of expert outputs.\"\"\"\n    if not self._fitted:\n        raise ValueError(\"MOE not fitted. Call fit() first.\")\n\n    X = np.asarray(X, dtype=np.float32)\n    X_jax = jnp.array(X, dtype=jnp.float32)\n\n    expert_preds = self._get_expert_predictions(X)\n    gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n    return np.sum(gating_weights * expert_preds, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.predict_with_uncertainty","title":"predict_with_uncertainty","text":"<pre><code>predict_with_uncertainty(\n    X: ndarray,\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Predict with epistemic uncertainty from expert disagreement.</p> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>Weighted mean predictions</p> <code>std</code> <code>ndarray</code> <p>Weighted standard deviation across experts</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def predict_with_uncertainty(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predict with epistemic uncertainty from expert disagreement.\n\n    Returns:\n        mean: Weighted mean predictions\n        std: Weighted standard deviation across experts\n    \"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    X_jax = jnp.array(X, dtype=jnp.float32)\n\n    expert_preds = self._get_expert_predictions(X)\n    weights = np.array(self.gating(self.gating_params, X_jax))\n\n    # Weighted mean\n    mean = np.sum(weights * expert_preds, axis=-1)\n\n    # Weighted variance: Var = E[f\u00b2] - E[f]\u00b2\n    mean_sq = np.sum(weights * expert_preds**2, axis=-1)\n    variance = mean_sq - mean**2\n    std = np.sqrt(np.maximum(variance, 0))\n\n    return mean, std\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.get_expert_weights","title":"get_expert_weights","text":"<pre><code>get_expert_weights(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Get gating weights (prior over experts) for inputs.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def get_expert_weights(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Get gating weights (prior over experts) for inputs.\"\"\"\n    X_jax = jnp.array(X, dtype=jnp.float32)\n    return np.array(self.gating(self.gating_params, X_jax))\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMMOE.get_responsibilities","title":"get_responsibilities","text":"<pre><code>get_responsibilities(X: ndarray, y: ndarray) -&gt; np.ndarray\n</code></pre> <p>Get expert responsibilities for given samples.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def get_responsibilities(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Get expert responsibilities for given samples.\"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    y = np.asarray(y, dtype=np.float32)\n\n    expert_preds = self._get_expert_predictions(X)\n    X_jax = jnp.array(X, dtype=jnp.float32)\n    gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n    return self._compute_responsibilities(X, y, expert_preds, gating_weights)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.EMConfig","title":"EMConfig  <code>dataclass</code>","text":"<p>Configuration for EM-based MOE training.</p> <p>Attributes:</p> Name Type Description <code>num_experts</code> <code>int</code> <p>Number of expert models.</p> <code>em_iterations</code> <code>int</code> <p>Number of EM iterations.</p> <code>gating_hidden_dims</code> <code>tuple[int, ...]</code> <p>Hidden layer dimensions for gating MLP.</p> <code>gating_epochs_per_iter</code> <code>int</code> <p>Gating training epochs per EM iteration.</p> <code>gating_lr</code> <code>float</code> <p>Learning rate for gating network.</p> <code>noise_variance</code> <code>float</code> <p>Assumed noise variance for likelihood (regression).</p> <code>expert_init_strategy</code> <code>Literal['random', 'cluster', 'bootstrap', 'uniform']</code> <p>How to initialize experts before EM. - \"random\": Random partition - \"cluster\": K-means clustering - \"bootstrap\": Bootstrap sampling - \"uniform\": Train all on full data (no partition)</p> <code>min_responsibility</code> <code>float</code> <p>Minimum responsibility threshold for expert training.</p> <code>temperature</code> <code>float</code> <p>Softmax temperature for gating.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>@dataclass  \nclass EMConfig:\n    \"\"\"Configuration for EM-based MOE training.\n\n    Attributes:\n        num_experts: Number of expert models.\n        em_iterations: Number of EM iterations.\n        gating_hidden_dims: Hidden layer dimensions for gating MLP.\n        gating_epochs_per_iter: Gating training epochs per EM iteration.\n        gating_lr: Learning rate for gating network.\n        noise_variance: Assumed noise variance for likelihood (regression).\n        expert_init_strategy: How to initialize experts before EM.\n            - \"random\": Random partition\n            - \"cluster\": K-means clustering  \n            - \"bootstrap\": Bootstrap sampling\n            - \"uniform\": Train all on full data (no partition)\n        min_responsibility: Minimum responsibility threshold for expert training.\n        temperature: Softmax temperature for gating.\n    \"\"\"\n    num_experts: int = 4\n    em_iterations: int = 10\n    gating_hidden_dims: tuple[int, ...] = (32, 16)\n    gating_epochs_per_iter: int = 100\n    gating_lr: float = 0.01\n    noise_variance: float = 1.0\n    expert_init_strategy: Literal[\"random\", \"cluster\", \"bootstrap\", \"uniform\"] = \"cluster\"\n    min_responsibility: float = 0.01\n    temperature: float = 1.0\n</code></pre>"},{"location":"api/ensemble/#variants","title":"Variants","text":""},{"location":"api/ensemble/#jaxboost.ensemble.HardEMMOE","title":"HardEMMOE","text":"<p>               Bases: <code>EMMOE</code></p> <p>Hard-EM variant where each sample is assigned to one expert.</p> <p>Instead of soft responsibilities, uses hard assignment:     z_i = argmax_k \u03b3_{ik}</p> <p>Then trains each expert only on samples assigned to it.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>class HardEMMOE(EMMOE):\n    \"\"\"Hard-EM variant where each sample is assigned to one expert.\n\n    Instead of soft responsibilities, uses hard assignment:\n        z_i = argmax_k \u03b3_{ik}\n\n    Then trains each expert only on samples assigned to it.\n    \"\"\"\n\n    def _m_step_experts(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        responsibilities: np.ndarray,\n    ) -&gt; None:\n        \"\"\"M-step with hard assignments.\"\"\"\n        assignments = responsibilities.argmax(axis=-1)  # (n_samples,)\n\n        for k in range(self.config.num_experts):\n            mask = assignments == k\n            if mask.sum() &gt; 0:\n                self.experts[k].fit(X[mask], y[mask])\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.SparseEMMOE","title":"SparseEMMOE","text":"<p>               Bases: <code>EMMOE</code></p> <p>EM-MOE with sparse top-k routing.</p> <p>Only top-k experts are activated per sample, reducing computation at inference time.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>class SparseEMMOE(EMMOE):\n    \"\"\"EM-MOE with sparse top-k routing.\n\n    Only top-k experts are activated per sample, reducing computation\n    at inference time.\n    \"\"\"\n\n    def __init__(\n        self,\n        experts: list[ExternalGBDTExpert],\n        config: EMConfig | None = None,\n        task: Literal[\"regression\", \"classification\"] = \"regression\",\n        top_k: int = 2,\n    ):\n        super().__init__(experts, config, task)\n        self.top_k = top_k\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict using only top-k experts per sample.\"\"\"\n        if not self._fitted:\n            raise ValueError(\"MOE not fitted. Call fit() first.\")\n\n        X = np.asarray(X, dtype=np.float32)\n        X_jax = jnp.array(X, dtype=jnp.float32)\n\n        expert_preds = self._get_expert_predictions(X)\n        gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n        # Keep only top-k weights\n        n_samples = X.shape[0]\n        sparse_weights = np.zeros_like(gating_weights)\n\n        for i in range(n_samples):\n            top_k_idx = np.argsort(gating_weights[i])[-self.top_k:]\n            sparse_weights[i, top_k_idx] = gating_weights[i, top_k_idx]\n            sparse_weights[i] /= sparse_weights[i].sum()\n\n        return np.sum(sparse_weights * expert_preds, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.SparseEMMOE.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict using only top-k experts per sample.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict using only top-k experts per sample.\"\"\"\n    if not self._fitted:\n        raise ValueError(\"MOE not fitted. Call fit() first.\")\n\n    X = np.asarray(X, dtype=np.float32)\n    X_jax = jnp.array(X, dtype=jnp.float32)\n\n    expert_preds = self._get_expert_predictions(X)\n    gating_weights = np.array(self.gating(self.gating_params, X_jax))\n\n    # Keep only top-k weights\n    n_samples = X.shape[0]\n    sparse_weights = np.zeros_like(gating_weights)\n\n    for i in range(n_samples):\n        top_k_idx = np.argsort(gating_weights[i])[-self.top_k:]\n        sparse_weights[i, top_k_idx] = gating_weights[i, top_k_idx]\n        sparse_weights[i] /= sparse_weights[i].sum()\n\n    return np.sum(sparse_weights * expert_preds, axis=-1)\n</code></pre>"},{"location":"api/ensemble/#expert-factories","title":"Expert Factories","text":""},{"location":"api/ensemble/#jaxboost.ensemble.create_xgboost_expert","title":"create_xgboost_expert","text":"<pre><code>create_xgboost_expert(**kwargs) -&gt; ExternalGBDTExpert\n</code></pre> <p>Create an XGBoost expert wrapper.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def create_xgboost_expert(**kwargs) -&gt; ExternalGBDTExpert:\n    \"\"\"Create an XGBoost expert wrapper.\"\"\"\n    try:\n        import xgboost as xgb\n    except ImportError:\n        raise ImportError(\"xgboost is required: pip install xgboost\")\n\n    class XGBExpert:\n        def __init__(self, task: str = \"regression\", **params):\n            self.task = task\n            self.params = params\n            self.model = None\n\n        def fit(self, X, y, sample_weight=None):\n            if self.task == \"regression\":\n                self.model = xgb.XGBRegressor(**self.params)\n            else:\n                self.model = xgb.XGBClassifier(**self.params)\n            self.model.fit(X, y, sample_weight=sample_weight)\n\n        def predict(self, X):\n            if self.model is None:\n                raise ValueError(\"Expert not fitted\")\n            if self.task == \"regression\":\n                return self.model.predict(X)\n            else:\n                return self.model.predict_proba(X)[:, 1]\n\n    return XGBExpert(**kwargs)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.create_lightgbm_expert","title":"create_lightgbm_expert","text":"<pre><code>create_lightgbm_expert(**kwargs) -&gt; ExternalGBDTExpert\n</code></pre> <p>Create a LightGBM expert wrapper.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def create_lightgbm_expert(**kwargs) -&gt; ExternalGBDTExpert:\n    \"\"\"Create a LightGBM expert wrapper.\"\"\"\n    try:\n        import lightgbm as lgb\n    except ImportError:\n        raise ImportError(\"lightgbm is required: pip install lightgbm\")\n\n    class LGBMExpert:\n        def __init__(self, task: str = \"regression\", **params):\n            self.task = task\n            self.params = {**params, \"verbosity\": -1}\n            self.model = None\n\n        def fit(self, X, y, sample_weight=None):\n            if self.task == \"regression\":\n                self.model = lgb.LGBMRegressor(**self.params)\n            else:\n                self.model = lgb.LGBMClassifier(**self.params)\n            self.model.fit(X, y, sample_weight=sample_weight)\n\n        def predict(self, X):\n            if self.model is None:\n                raise ValueError(\"Expert not fitted\")\n            if self.task == \"regression\":\n                return self.model.predict(X)\n            else:\n                return self.model.predict_proba(X)[:, 1]\n\n    return LGBMExpert(**kwargs)\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.create_catboost_expert","title":"create_catboost_expert","text":"<pre><code>create_catboost_expert(**kwargs) -&gt; ExternalGBDTExpert\n</code></pre> <p>Create a CatBoost expert wrapper.</p> Source code in <code>src/jaxboost/ensemble/hybrid_moe.py</code> <pre><code>def create_catboost_expert(**kwargs) -&gt; ExternalGBDTExpert:\n    \"\"\"Create a CatBoost expert wrapper.\"\"\"\n    try:\n        from catboost import CatBoostRegressor, CatBoostClassifier\n    except ImportError:\n        raise ImportError(\"catboost is required: pip install catboost\")\n\n    class CatBoostExpert:\n        def __init__(self, task: str = \"regression\", **params):\n            self.task = task\n            self.params = {**params, \"verbose\": False}\n            self.model = None\n\n        def fit(self, X, y, sample_weight=None):\n            if self.task == \"regression\":\n                self.model = CatBoostRegressor(**self.params)\n            else:\n                self.model = CatBoostClassifier(**self.params)\n            self.model.fit(X, y, sample_weight=sample_weight)\n\n        def predict(self, X):\n            if self.model is None:\n                raise ValueError(\"Expert not fitted\")\n            if self.task == \"regression\":\n                return self.model.predict(X)\n            else:\n                return self.model.predict_proba(X)[:, 1]\n\n    return CatBoostExpert(**kwargs)\n</code></pre>"},{"location":"api/ensemble/#utilities","title":"Utilities","text":""},{"location":"api/ensemble/#jaxboost.ensemble.sparse_top_k","title":"sparse_top_k","text":"<pre><code>sparse_top_k(\n    gate_weights: Array, k: int\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Apply top-k sparsity to gate weights.</p> <p>Only the top-k experts are activated, others are zeroed out. Weights are renormalized after selection.</p> <p>Parameters:</p> Name Type Description Default <code>gate_weights</code> <code>Array</code> <p>Dense gate weights, shape (batch, num_experts).</p> required <code>k</code> <code>int</code> <p>Number of experts to keep.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Tuple of:</p> <code>Array</code> <ul> <li>sparse_weights: Sparse gate weights, shape (batch, num_experts).</li> </ul> <code>tuple[Array, Array]</code> <ul> <li>top_k_indices: Indices of selected experts, shape (batch, k).</li> </ul> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def sparse_top_k(\n    gate_weights: Array,\n    k: int,\n) -&gt; tuple[Array, Array]:\n    \"\"\"Apply top-k sparsity to gate weights.\n\n    Only the top-k experts are activated, others are zeroed out.\n    Weights are renormalized after selection.\n\n    Args:\n        gate_weights: Dense gate weights, shape (batch, num_experts).\n        k: Number of experts to keep.\n\n    Returns:\n        Tuple of:\n        - sparse_weights: Sparse gate weights, shape (batch, num_experts).\n        - top_k_indices: Indices of selected experts, shape (batch, k).\n    \"\"\"\n    batch_size, num_experts = gate_weights.shape\n\n    # Get top-k indices and values\n    top_k_values, top_k_indices = jax.lax.top_k(gate_weights, k)\n\n    # Renormalize top-k values\n    top_k_probs = top_k_values / (jnp.sum(top_k_values, axis=-1, keepdims=True) + 1e-8)\n\n    # Scatter back to sparse tensor\n    sparse_weights = jnp.zeros_like(gate_weights)\n    batch_idx = jnp.arange(batch_size)[:, None]  # (batch, 1)\n    sparse_weights = sparse_weights.at[batch_idx, top_k_indices].set(top_k_probs)\n\n    return sparse_weights, top_k_indices\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.load_balance_loss","title":"load_balance_loss","text":"<pre><code>load_balance_loss(gate_weights: Array) -&gt; Array\n</code></pre> <p>Compute load balancing loss to encourage uniform expert usage.</p> <p>Penalizes variance in expert load across the batch.</p> <p>Parameters:</p> Name Type Description Default <code>gate_weights</code> <code>Array</code> <p>Gate weights, shape (batch, num_experts).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Load balance loss (scalar). Lower = more balanced.</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def load_balance_loss(gate_weights: Array) -&gt; Array:\n    \"\"\"Compute load balancing loss to encourage uniform expert usage.\n\n    Penalizes variance in expert load across the batch.\n\n    Args:\n        gate_weights: Gate weights, shape (batch, num_experts).\n\n    Returns:\n        Load balance loss (scalar). Lower = more balanced.\n    \"\"\"\n    # Average load per expert\n    expert_load = jnp.mean(gate_weights, axis=0)  # (num_experts,)\n\n    # Variance penalty (0 when perfectly balanced)\n    num_experts = gate_weights.shape[1]\n    return jnp.var(expert_load) * num_experts\n</code></pre>"},{"location":"api/ensemble/#jaxboost.ensemble.router_z_loss","title":"router_z_loss","text":"<pre><code>router_z_loss(gate_logits: Array) -&gt; Array\n</code></pre> <p>Compute router z-loss for training stability.</p> <p>Penalizes large logits to prevent router from becoming too confident. From \"ST-MoE: Designing Stable and Transferable Sparse Expert Models\".</p> <p>Parameters:</p> Name Type Description Default <code>gate_logits</code> <code>Array</code> <p>Raw gate logits before softmax, shape (batch, num_experts).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Z-loss (scalar).</p> Source code in <code>src/jaxboost/ensemble/gating.py</code> <pre><code>def router_z_loss(gate_logits: Array) -&gt; Array:\n    \"\"\"Compute router z-loss for training stability.\n\n    Penalizes large logits to prevent router from becoming too confident.\n    From \"ST-MoE: Designing Stable and Transferable Sparse Expert Models\".\n\n    Args:\n        gate_logits: Raw gate logits before softmax, shape (batch, num_experts).\n\n    Returns:\n        Z-loss (scalar).\n    \"\"\"\n    # Log-sum-exp of logits\n    log_z = jax.scipy.special.logsumexp(gate_logits, axis=-1)\n    return jnp.mean(log_z ** 2)\n</code></pre>"},{"location":"api/ensemble/#examples","title":"Examples","text":"<pre><code>from jaxboost.ensemble import EMMOE, EMConfig, create_xgboost_expert\n\nexperts = [\n    create_xgboost_expert(task=\"regression\", n_estimators=100, max_depth=4)\n    for _ in range(4)\n]\n\nconfig = EMConfig(\n    num_experts=4,\n    em_iterations=10,\n    expert_init_strategy=\"cluster\",\n)\n\nmoe = EMMOE(experts, config=config)\nmoe.fit(X_train, y_train)\n\nmean, std = moe.predict_with_uncertainty(X_test)\n</code></pre> <pre><code>from jaxboost.ensemble import MOEEnsemble\n\nmoe = MOEEnsemble(\n    num_experts=4,\n    trees_per_expert=10,\n    tree_depth=4,\n    gating=\"tree\",\n    task=\"regression\",\n)\n\nparams = moe.fit(X_train, y_train, epochs=300)\npredictions = moe.predict(params, X_test)\n</code></pre>"},{"location":"api/losses/","title":"Losses","text":"<p>Loss functions for training.</p>"},{"location":"api/losses/#regression-losses","title":"Regression Losses","text":""},{"location":"api/losses/#jaxboost.losses.regression","title":"regression","text":"<p>Regression loss functions.</p>"},{"location":"api/losses/#jaxboost.losses.regression.mse_loss","title":"mse_loss","text":"<pre><code>mse_loss(predictions: Array, targets: Array) -&gt; Array\n</code></pre> <p>Mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Array</code> <p>Model predictions, shape (batch,).</p> required <code>targets</code> <code>Array</code> <p>Ground truth targets, shape (batch,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar mean squared error.</p> Source code in <code>src/jaxboost/losses/regression.py</code> <pre><code>def mse_loss(predictions: Array, targets: Array) -&gt; Array:\n    \"\"\"Mean squared error loss.\n\n    Args:\n        predictions: Model predictions, shape (batch,).\n        targets: Ground truth targets, shape (batch,).\n\n    Returns:\n        Scalar mean squared error.\n    \"\"\"\n    return jnp.mean((predictions - targets) ** 2)\n</code></pre>"},{"location":"api/losses/#classification-losses","title":"Classification Losses","text":""},{"location":"api/losses/#jaxboost.losses.classification","title":"classification","text":"<p>Classification loss functions.</p>"},{"location":"api/losses/#jaxboost.losses.classification.sigmoid_binary_cross_entropy","title":"sigmoid_binary_cross_entropy","text":"<pre><code>sigmoid_binary_cross_entropy(\n    logits: Array, targets: Array\n) -&gt; Array\n</code></pre> <p>Binary cross-entropy loss with logits (numerically stable).</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>Raw logits (before sigmoid), shape (batch,).</p> required <code>targets</code> <code>Array</code> <p>Binary targets (0 or 1), shape (batch,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar binary cross-entropy loss.</p> Source code in <code>src/jaxboost/losses/classification.py</code> <pre><code>def sigmoid_binary_cross_entropy(logits: Array, targets: Array) -&gt; Array:\n    \"\"\"Binary cross-entropy loss with logits (numerically stable).\n\n    Args:\n        logits: Raw logits (before sigmoid), shape (batch,).\n        targets: Binary targets (0 or 1), shape (batch,).\n\n    Returns:\n        Scalar binary cross-entropy loss.\n    \"\"\"\n    # Numerically stable BCE:\n    # max(logits, 0) - logits * targets + log(1 + exp(-|logits|))\n    loss = (\n        jnp.maximum(logits, 0)\n        - logits * targets\n        + jnp.log1p(jnp.exp(-jnp.abs(logits)))\n    )\n    return jnp.mean(loss)\n</code></pre>"},{"location":"api/routing/","title":"Routing","text":"<p>Soft routing functions for differentiable trees.</p>"},{"location":"api/routing/#jaxboost.routing.soft","title":"soft","text":"<p>Soft (sigmoid) routing function.</p> <p>Converts split scores to probabilities using sigmoid function. This enables gradient flow through the tree structure.</p>"},{"location":"api/routing/#jaxboost.routing.soft.soft_routing","title":"soft_routing","text":"<pre><code>soft_routing(\n    score: Array, temperature: float = 1.0\n) -&gt; Array\n</code></pre> <p>Soft routing using sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Array</code> <p>Split scores, any shape.</p> required <code>temperature</code> <code>float</code> <p>Sharpness parameter. Higher = softer, Lower = sharper.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Probability of going right, same shape as score, values in [0, 1].</p> Source code in <code>src/jaxboost/routing/soft.py</code> <pre><code>def soft_routing(score: Array, temperature: float = 1.0) -&gt; Array:\n    \"\"\"Soft routing using sigmoid function.\n\n    Args:\n        score: Split scores, any shape.\n        temperature: Sharpness parameter. Higher = softer, Lower = sharper.\n\n    Returns:\n        Probability of going right, same shape as score, values in [0, 1].\n    \"\"\"\n    return jax.nn.sigmoid(score * temperature)\n</code></pre>"},{"location":"api/splits/","title":"Splits","text":"<p>Split mechanisms for decision trees.</p>"},{"location":"api/splits/#axis-aligned-splits","title":"Axis-Aligned Splits","text":""},{"location":"api/splits/#jaxboost.splits.axis_aligned","title":"axis_aligned","text":"<p>Axis-aligned split function (soft version).</p> <p>Uses softmax for differentiable feature selection.</p>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit","title":"AxisAlignedSplit","text":"<p>Soft axis-aligned split using softmax feature selection.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>class AxisAlignedSplit:\n    \"\"\"Soft axis-aligned split using softmax feature selection.\"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        init_threshold_scale: float = 0.1,\n    ) -&gt; AxisAlignedSplitParams:\n        \"\"\"Initialize split parameters.\"\"\"\n        keys = jax.random.split(key, 2)\n        feature_logits = jax.random.normal(keys[0], (num_features,)) * 0.01\n        threshold = jax.random.normal(keys[1], ()) * init_threshold_scale\n        return AxisAlignedSplitParams(feature_logits=feature_logits, threshold=threshold)\n\n    def compute_score(self, params: AxisAlignedSplitParams, x: Array) -&gt; Array:\n        \"\"\"Compute split score. Positive \u2192 right, Negative \u2192 left.\"\"\"\n        feature_probs = jax.nn.softmax(params.feature_logits)\n        selected_value = jnp.sum(x * feature_probs, axis=-1)\n        return selected_value - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    num_features: int,\n    init_threshold_scale: float = 0.1,\n) -&gt; AxisAlignedSplitParams\n</code></pre> <p>Initialize split parameters.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    init_threshold_scale: float = 0.1,\n) -&gt; AxisAlignedSplitParams:\n    \"\"\"Initialize split parameters.\"\"\"\n    keys = jax.random.split(key, 2)\n    feature_logits = jax.random.normal(keys[0], (num_features,)) * 0.01\n    threshold = jax.random.normal(keys[1], ()) * init_threshold_scale\n    return AxisAlignedSplitParams(feature_logits=feature_logits, threshold=threshold)\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplit.compute_score","title":"compute_score","text":"<pre><code>compute_score(\n    params: AxisAlignedSplitParams, x: Array\n) -&gt; Array\n</code></pre> <p>Compute split score. Positive \u2192 right, Negative \u2192 left.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>def compute_score(self, params: AxisAlignedSplitParams, x: Array) -&gt; Array:\n    \"\"\"Compute split score. Positive \u2192 right, Negative \u2192 left.\"\"\"\n    feature_probs = jax.nn.softmax(params.feature_logits)\n    selected_value = jnp.sum(x * feature_probs, axis=-1)\n    return selected_value - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.axis_aligned.AxisAlignedSplitParams","title":"AxisAlignedSplitParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for a soft axis-aligned split.</p> <p>Attributes:</p> Name Type Description <code>feature_logits</code> <code>Array</code> <p>Soft feature selection logits, shape (num_features,).</p> <code>threshold</code> <code>Array</code> <p>Split threshold, scalar.</p> Source code in <code>src/jaxboost/splits/axis_aligned.py</code> <pre><code>class AxisAlignedSplitParams(NamedTuple):\n    \"\"\"Parameters for a soft axis-aligned split.\n\n    Attributes:\n        feature_logits: Soft feature selection logits, shape (num_features,).\n        threshold: Split threshold, scalar.\n    \"\"\"\n\n    feature_logits: Array  # (num_features,)\n    threshold: Array  # scalar\n</code></pre>"},{"location":"api/splits/#hyperplane-splits","title":"Hyperplane Splits","text":""},{"location":"api/splits/#jaxboost.splits.hyperplane","title":"hyperplane","text":"<p>Hyperplane split function.</p> <p>Instead of axis-aligned splits (x[j] &lt;= t), hyperplane splits use a learned linear combination: w @ x &lt;= t</p> <p>This captures feature interactions directly in a single split!</p>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit","title":"HyperplaneSplit","text":"<p>Hyperplane split: w @ x - threshold.</p> <p>More expressive than axis-aligned splits. A single hyperplane split can capture linear feature interactions.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>class HyperplaneSplit:\n    \"\"\"Hyperplane split: w @ x - threshold.\n\n    More expressive than axis-aligned splits.\n    A single hyperplane split can capture linear feature interactions.\n    \"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        num_features: int,\n        init_scale: float = 0.1,\n    ) -&gt; HyperplaneSplitParams:\n        \"\"\"Initialize split parameters.\n\n        Weights are initialized small and normalized.\n        \"\"\"\n        keys = jax.random.split(key, 2)\n        weights = jax.random.normal(keys[0], (num_features,)) * init_scale\n        weights = weights / (jnp.linalg.norm(weights) + 1e-8)\n        threshold = jax.random.normal(keys[1], ()) * 0.1\n        return HyperplaneSplitParams(weights=weights, threshold=threshold)\n\n    def compute_score(self, params: HyperplaneSplitParams, x: Array) -&gt; Array:\n        \"\"\"Compute split score: w @ x - threshold.\n\n        Positive \u2192 go right, Negative \u2192 go left.\n        \"\"\"\n        # x: (..., num_features), weights: (num_features,)\n        projection = jnp.sum(x * params.weights, axis=-1)\n        return projection - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array, num_features: int, init_scale: float = 0.1\n) -&gt; HyperplaneSplitParams\n</code></pre> <p>Initialize split parameters.</p> <p>Weights are initialized small and normalized.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    num_features: int,\n    init_scale: float = 0.1,\n) -&gt; HyperplaneSplitParams:\n    \"\"\"Initialize split parameters.\n\n    Weights are initialized small and normalized.\n    \"\"\"\n    keys = jax.random.split(key, 2)\n    weights = jax.random.normal(keys[0], (num_features,)) * init_scale\n    weights = weights / (jnp.linalg.norm(weights) + 1e-8)\n    threshold = jax.random.normal(keys[1], ()) * 0.1\n    return HyperplaneSplitParams(weights=weights, threshold=threshold)\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplit.compute_score","title":"compute_score","text":"<pre><code>compute_score(\n    params: HyperplaneSplitParams, x: Array\n) -&gt; Array\n</code></pre> <p>Compute split score: w @ x - threshold.</p> <p>Positive \u2192 go right, Negative \u2192 go left.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>def compute_score(self, params: HyperplaneSplitParams, x: Array) -&gt; Array:\n    \"\"\"Compute split score: w @ x - threshold.\n\n    Positive \u2192 go right, Negative \u2192 go left.\n    \"\"\"\n    # x: (..., num_features), weights: (num_features,)\n    projection = jnp.sum(x * params.weights, axis=-1)\n    return projection - params.threshold\n</code></pre>"},{"location":"api/splits/#jaxboost.splits.hyperplane.HyperplaneSplitParams","title":"HyperplaneSplitParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for a hyperplane split.</p> <p>Attributes:</p> Name Type Description <code>weights</code> <code>Array</code> <p>Linear combination weights, shape (num_features,).</p> <code>threshold</code> <code>Array</code> <p>Split threshold, scalar.</p> Source code in <code>src/jaxboost/splits/hyperplane.py</code> <pre><code>class HyperplaneSplitParams(NamedTuple):\n    \"\"\"Parameters for a hyperplane split.\n\n    Attributes:\n        weights: Linear combination weights, shape (num_features,).\n        threshold: Split threshold, scalar.\n    \"\"\"\n\n    weights: Array  # (num_features,)\n    threshold: Array  # scalar\n</code></pre>"},{"location":"api/structures/","title":"Structures","text":"<p>Tree structures for gradient boosting.</p>"},{"location":"api/structures/#oblivious-trees","title":"Oblivious Trees","text":""},{"location":"api/structures/#jaxboost.structures.oblivious","title":"oblivious","text":"<p>Oblivious tree structure.</p> <p>An oblivious tree uses the same split at each depth level. This makes it highly efficient for GPU computation.</p> <p>Key property: For depth D, there are exactly 2^D leaves.</p>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree","title":"ObliviousTree","text":"<p>Oblivious (symmetric) tree structure.</p> <p>All nodes at the same depth share the same split. This enables efficient vectorized computation on GPU.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>class ObliviousTree:\n    \"\"\"Oblivious (symmetric) tree structure.\n\n    All nodes at the same depth share the same split.\n    This enables efficient vectorized computation on GPU.\n    \"\"\"\n\n    def init_params(\n        self,\n        key: Array,\n        depth: int,\n        num_features: int,\n        split_fn: SplitFn[Any],\n        init_leaf_scale: float = 0.01,\n    ) -&gt; ObliviousTreeParams:\n        \"\"\"Initialize tree parameters.\n\n        Args:\n            key: JAX PRNG key.\n            depth: Tree depth (number of split levels).\n            num_features: Number of input features.\n            split_fn: Split function to use.\n            init_leaf_scale: Scale for leaf value initialization.\n\n        Returns:\n            Initialized tree parameters.\n        \"\"\"\n        num_leaves = 2**depth\n        keys = jax.random.split(key, depth + 1)\n\n        split_params = [\n            split_fn.init_params(keys[d], num_features) for d in range(depth)\n        ]\n        leaf_values = jax.random.normal(keys[depth], (num_leaves,)) * init_leaf_scale\n\n        return ObliviousTreeParams(split_params=split_params, leaf_values=leaf_values)\n\n    def forward(\n        self,\n        params: ObliviousTreeParams,\n        x: Array,\n        split_fn: SplitFn[Any],\n        routing_fn: RoutingFn,\n    ) -&gt; Array:\n        \"\"\"Forward pass through the tree.\n\n        Args:\n            params: Tree parameters.\n            x: Input features, shape (batch, num_features) or (num_features,).\n            split_fn: Split function.\n            routing_fn: Routing function.\n\n        Returns:\n            Predictions, shape (batch,) or scalar.\n        \"\"\"\n        single_sample = x.ndim == 1\n        if single_sample:\n            x = x[None, :]\n\n        depth = len(params.split_params)\n        p_rights = []\n        for d in range(depth):\n            score = split_fn.compute_score(params.split_params[d], x)\n            p_rights.append(routing_fn(score))\n        p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n        leaf_probs = self._compute_leaf_probs(p_rights, depth)\n        output = jnp.sum(leaf_probs * params.leaf_values, axis=-1)\n\n        return output[0] if single_sample else output\n\n    def _compute_leaf_probs(self, p_rights: Array, depth: int) -&gt; Array:\n        \"\"\"Compute probability of reaching each leaf (fully vectorized).\n\n        Uses bit manipulation + broadcasting - no Python loops!\n\n        Each leaf index's binary representation encodes the path:\n        bit d = 0 means go left, bit d = 1 means go right at depth d.\n        \"\"\"\n        num_leaves = 2**depth\n\n        # Create binary mask: which way to go at each depth for each leaf\n        leaf_indices = jnp.arange(num_leaves)\n        depth_indices = jnp.arange(depth)\n\n        # bit_mask[leaf, depth] = 1 if leaf goes right at depth, else 0\n        bit_mask = (leaf_indices[:, None] &gt;&gt; depth_indices) &amp; 1  # (num_leaves, depth)\n\n        # p_rights: (depth, batch) -&gt; (batch, depth)\n        p_rights_T = p_rights.T  # (batch, depth)\n\n        # Broadcast: (batch, 1, depth) vs (1, num_leaves, depth)\n        p_rights_expanded = p_rights_T[:, None, :]  # (batch, 1, depth)\n        bit_mask_expanded = bit_mask[None, :, :]  # (1, num_leaves, depth)\n\n        # routing_prob = p_right if go_right else (1 - p_right)\n        routing_probs = (\n            bit_mask_expanded * p_rights_expanded\n            + (1 - bit_mask_expanded) * (1 - p_rights_expanded)\n        )  # (batch, num_leaves, depth)\n\n        # Product over depth dimension\n        leaf_probs = jnp.prod(routing_probs, axis=-1)  # (batch, num_leaves)\n\n        return leaf_probs\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree.init_params","title":"init_params","text":"<pre><code>init_params(\n    key: Array,\n    depth: int,\n    num_features: int,\n    split_fn: SplitFn[Any],\n    init_leaf_scale: float = 0.01,\n) -&gt; ObliviousTreeParams\n</code></pre> <p>Initialize tree parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX PRNG key.</p> required <code>depth</code> <code>int</code> <p>Tree depth (number of split levels).</p> required <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>split_fn</code> <code>SplitFn[Any]</code> <p>Split function to use.</p> required <code>init_leaf_scale</code> <code>float</code> <p>Scale for leaf value initialization.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>ObliviousTreeParams</code> <p>Initialized tree parameters.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>def init_params(\n    self,\n    key: Array,\n    depth: int,\n    num_features: int,\n    split_fn: SplitFn[Any],\n    init_leaf_scale: float = 0.01,\n) -&gt; ObliviousTreeParams:\n    \"\"\"Initialize tree parameters.\n\n    Args:\n        key: JAX PRNG key.\n        depth: Tree depth (number of split levels).\n        num_features: Number of input features.\n        split_fn: Split function to use.\n        init_leaf_scale: Scale for leaf value initialization.\n\n    Returns:\n        Initialized tree parameters.\n    \"\"\"\n    num_leaves = 2**depth\n    keys = jax.random.split(key, depth + 1)\n\n    split_params = [\n        split_fn.init_params(keys[d], num_features) for d in range(depth)\n    ]\n    leaf_values = jax.random.normal(keys[depth], (num_leaves,)) * init_leaf_scale\n\n    return ObliviousTreeParams(split_params=split_params, leaf_values=leaf_values)\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTree.forward","title":"forward","text":"<pre><code>forward(\n    params: ObliviousTreeParams,\n    x: Array,\n    split_fn: SplitFn[Any],\n    routing_fn: RoutingFn,\n) -&gt; Array\n</code></pre> <p>Forward pass through the tree.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ObliviousTreeParams</code> <p>Tree parameters.</p> required <code>x</code> <code>Array</code> <p>Input features, shape (batch, num_features) or (num_features,).</p> required <code>split_fn</code> <code>SplitFn[Any]</code> <p>Split function.</p> required <code>routing_fn</code> <code>RoutingFn</code> <p>Routing function.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Predictions, shape (batch,) or scalar.</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>def forward(\n    self,\n    params: ObliviousTreeParams,\n    x: Array,\n    split_fn: SplitFn[Any],\n    routing_fn: RoutingFn,\n) -&gt; Array:\n    \"\"\"Forward pass through the tree.\n\n    Args:\n        params: Tree parameters.\n        x: Input features, shape (batch, num_features) or (num_features,).\n        split_fn: Split function.\n        routing_fn: Routing function.\n\n    Returns:\n        Predictions, shape (batch,) or scalar.\n    \"\"\"\n    single_sample = x.ndim == 1\n    if single_sample:\n        x = x[None, :]\n\n    depth = len(params.split_params)\n    p_rights = []\n    for d in range(depth):\n        score = split_fn.compute_score(params.split_params[d], x)\n        p_rights.append(routing_fn(score))\n    p_rights = jnp.stack(p_rights, axis=0)  # (depth, batch)\n\n    leaf_probs = self._compute_leaf_probs(p_rights, depth)\n    output = jnp.sum(leaf_probs * params.leaf_values, axis=-1)\n\n    return output[0] if single_sample else output\n</code></pre>"},{"location":"api/structures/#jaxboost.structures.oblivious.ObliviousTreeParams","title":"ObliviousTreeParams","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for an oblivious tree.</p> <p>Attributes:</p> Name Type Description <code>split_params</code> <code>list[Any]</code> <p>Split parameters, one per depth level.</p> <code>leaf_values</code> <code>Array</code> <p>Prediction values at leaves, shape (2^depth,).</p> Source code in <code>src/jaxboost/structures/oblivious.py</code> <pre><code>class ObliviousTreeParams(NamedTuple):\n    \"\"\"Parameters for an oblivious tree.\n\n    Attributes:\n        split_params: Split parameters, one per depth level.\n        leaf_values: Prediction values at leaves, shape (2^depth,).\n    \"\"\"\n\n    split_params: list[Any]  # length = depth\n    leaf_values: Array  # (2^depth,)\n</code></pre>"},{"location":"api/training/","title":"Training","text":"<p>High-level training API for JAXBoost.</p>"},{"location":"api/training/#jaxboost.training","title":"training","text":"<p>Training utilities for jaxboost.</p>"},{"location":"api/training/#jaxboost.training.GBMTrainer","title":"GBMTrainer","text":"<p>Gradient boosting trainer with soft oblivious trees.</p> Example <p>trainer = GBMTrainer(task=\"regression\") model = trainer.fit(X_train, y_train) predictions = model.predict(X_test)</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>class GBMTrainer:\n    \"\"\"Gradient boosting trainer with soft oblivious trees.\n\n    Example:\n        &gt;&gt;&gt; trainer = GBMTrainer(task=\"regression\")\n        &gt;&gt;&gt; model = trainer.fit(X_train, y_train)\n        &gt;&gt;&gt; predictions = model.predict(X_test)\n    \"\"\"\n\n    def __init__(\n        self,\n        task: Literal[\"regression\", \"classification\"] = \"regression\",\n        config: TrainerConfig | None = None,\n    ):\n        \"\"\"Initialize trainer.\n\n        Args:\n            task: 'regression' or 'classification'.\n            config: Training configuration. If None, uses defaults.\n        \"\"\"\n        self.task = task\n        self.config = config or TrainerConfig()\n        self.split_fn = HyperplaneSplit()\n        self.tree = ObliviousTree()\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        X_val: np.ndarray | None = None,\n        y_val: np.ndarray | None = None,\n    ) -&gt; TrainedGBM:\n        \"\"\"Train boosted trees on the given data.\n\n        Args:\n            X: Training features, shape (n_samples, n_features).\n            y: Training targets, shape (n_samples,).\n            X_val: Optional validation features.\n            y_val: Optional validation targets.\n\n        Returns:\n            Trained model.\n        \"\"\"\n        config = self.config\n        n_samples, n_features = X.shape\n\n        # Auto-adjust hyperparameters based on data size\n        config = self._adapt_config(config, n_samples)\n\n        # Create validation split if not provided\n        if X_val is None:\n            split_idx = int(n_samples * (1 - config.val_fraction))\n            indices = np.random.RandomState(42).permutation(n_samples)\n            train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n        else:\n            X_train, y_train = X, y\n\n        # Normalize target for regression\n        if self.task == \"regression\" and config.normalize_target:\n            target_mean, target_std = float(y_train.mean()), float(y_train.std())\n            y_train = (y_train - target_mean) / target_std\n            y_val = (y_val - target_mean) / target_std\n        else:\n            target_mean, target_std = 0.0, 1.0\n\n        # Convert to JAX arrays\n        X_train = jnp.array(X_train, dtype=jnp.float32)\n        y_train = jnp.array(y_train, dtype=jnp.float32)\n        X_val = jnp.array(X_val, dtype=jnp.float32)\n        y_val = jnp.array(y_val, dtype=jnp.float32)\n\n        # Initialize trees\n        key = jax.random.PRNGKey(42)\n        ensemble_params = []\n        for _ in range(config.n_trees):\n            key, subkey = jax.random.split(key)\n            params = self.tree.init_params(\n                subkey, config.depth, n_features, self.split_fn\n            )\n            ensemble_params.append(params)\n\n        # Setup optimizer\n        schedule = optax.cosine_decay_schedule(\n            config.learning_rate, config.epochs, alpha=0.01\n        )\n        optimizer = optax.adam(schedule)\n        opt_state = optimizer.init(ensemble_params)\n\n        # Training loop with early stopping\n        best_val_loss = float('inf')\n        best_params = ensemble_params\n        no_improve = 0\n\n        @jax.jit\n        def train_step(params, opt_state, X, y, temperature):\n            loss, grads = jax.value_and_grad(self._loss_fn)(\n                params, X, y, temperature, config\n            )\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss\n\n        for epoch in range(config.epochs):\n            progress = epoch / config.epochs\n            temperature = config.temp_start + progress * (config.temp_end - config.temp_start)\n\n            ensemble_params, opt_state, train_loss = train_step(\n                ensemble_params, opt_state, X_train, y_train, temperature\n            )\n\n            # Validation check\n            if epoch % 10 == 0:\n                val_loss = self._loss_fn(\n                    ensemble_params, X_val, y_val, temperature, config\n                )\n\n                if val_loss &lt; best_val_loss:\n                    best_val_loss = val_loss\n                    best_params = ensemble_params\n                    no_improve = 0\n                else:\n                    no_improve += 10\n\n                if config.verbose and epoch % 50 == 0:\n                    print(f\"  Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n\n                if no_improve &gt;= config.patience:\n                    if config.verbose:\n                        print(f\"  Early stopping at epoch {epoch}\")\n                    break\n\n        return TrainedGBM(\n            params=best_params,\n            config=config,\n            target_mean=target_mean,\n            target_std=target_std,\n            task=self.task,\n            split_fn=self.split_fn,\n            tree=self.tree,\n        )\n\n    def _adapt_config(self, config: TrainerConfig, n_samples: int) -&gt; TrainerConfig:\n        \"\"\"Adapt configuration based on dataset size.\"\"\"\n        if config.temp_end is None:\n            if n_samples &lt; 500:\n                temp_end = 3.0\n            elif n_samples &lt; 5000:\n                temp_end = 5.0\n            else:\n                temp_end = 10.0\n        else:\n            temp_end = config.temp_end\n\n        depth = config.depth\n        lr = config.learning_rate\n        if n_samples &lt; 500:\n            depth = min(depth, 3)\n            lr = min(lr, 0.01)\n\n        return TrainerConfig(\n            n_trees=config.n_trees,\n            depth=depth,\n            learning_rate=lr,\n            tree_weight=config.tree_weight,\n            epochs=config.epochs,\n            patience=config.patience,\n            temp_start=config.temp_start,\n            temp_end=temp_end,\n            val_fraction=config.val_fraction,\n            normalize_target=config.normalize_target,\n            verbose=config.verbose,\n        )\n\n    def _loss_fn(\n        self,\n        params: list,\n        X: Array,\n        y: Array,\n        temperature: float,\n        config: TrainerConfig,\n    ) -&gt; Array:\n        \"\"\"Compute loss for the ensemble.\"\"\"\n        def routing_fn(score):\n            return soft_routing(score, temperature)\n\n        preds = []\n        for tree_params in params:\n            pred = self.tree.forward(tree_params, X, self.split_fn, routing_fn)\n            preds.append(pred)\n\n        preds = jnp.stack(preds, axis=0)\n        weights = jnp.full((config.n_trees,), config.tree_weight)\n        output = boosting_aggregate(preds, weights)\n\n        if self.task == \"regression\":\n            return mse_loss(output, y)\n        else:\n            return sigmoid_binary_cross_entropy(output, y)\n</code></pre>"},{"location":"api/training/#jaxboost.training.GBMTrainer.__init__","title":"__init__","text":"<pre><code>__init__(\n    task: Literal[\n        \"regression\", \"classification\"\n    ] = \"regression\",\n    config: TrainerConfig | None = None,\n)\n</code></pre> <p>Initialize trainer.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Literal['regression', 'classification']</code> <p>'regression' or 'classification'.</p> <code>'regression'</code> <code>config</code> <code>TrainerConfig | None</code> <p>Training configuration. If None, uses defaults.</p> <code>None</code> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>def __init__(\n    self,\n    task: Literal[\"regression\", \"classification\"] = \"regression\",\n    config: TrainerConfig | None = None,\n):\n    \"\"\"Initialize trainer.\n\n    Args:\n        task: 'regression' or 'classification'.\n        config: Training configuration. If None, uses defaults.\n    \"\"\"\n    self.task = task\n    self.config = config or TrainerConfig()\n    self.split_fn = HyperplaneSplit()\n    self.tree = ObliviousTree()\n</code></pre>"},{"location":"api/training/#jaxboost.training.GBMTrainer.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    X_val: ndarray | None = None,\n    y_val: ndarray | None = None,\n) -&gt; TrainedGBM\n</code></pre> <p>Train boosted trees on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training features, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> required <code>X_val</code> <code>ndarray | None</code> <p>Optional validation features.</p> <code>None</code> <code>y_val</code> <code>ndarray | None</code> <p>Optional validation targets.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainedGBM</code> <p>Trained model.</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    X_val: np.ndarray | None = None,\n    y_val: np.ndarray | None = None,\n) -&gt; TrainedGBM:\n    \"\"\"Train boosted trees on the given data.\n\n    Args:\n        X: Training features, shape (n_samples, n_features).\n        y: Training targets, shape (n_samples,).\n        X_val: Optional validation features.\n        y_val: Optional validation targets.\n\n    Returns:\n        Trained model.\n    \"\"\"\n    config = self.config\n    n_samples, n_features = X.shape\n\n    # Auto-adjust hyperparameters based on data size\n    config = self._adapt_config(config, n_samples)\n\n    # Create validation split if not provided\n    if X_val is None:\n        split_idx = int(n_samples * (1 - config.val_fraction))\n        indices = np.random.RandomState(42).permutation(n_samples)\n        train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n    else:\n        X_train, y_train = X, y\n\n    # Normalize target for regression\n    if self.task == \"regression\" and config.normalize_target:\n        target_mean, target_std = float(y_train.mean()), float(y_train.std())\n        y_train = (y_train - target_mean) / target_std\n        y_val = (y_val - target_mean) / target_std\n    else:\n        target_mean, target_std = 0.0, 1.0\n\n    # Convert to JAX arrays\n    X_train = jnp.array(X_train, dtype=jnp.float32)\n    y_train = jnp.array(y_train, dtype=jnp.float32)\n    X_val = jnp.array(X_val, dtype=jnp.float32)\n    y_val = jnp.array(y_val, dtype=jnp.float32)\n\n    # Initialize trees\n    key = jax.random.PRNGKey(42)\n    ensemble_params = []\n    for _ in range(config.n_trees):\n        key, subkey = jax.random.split(key)\n        params = self.tree.init_params(\n            subkey, config.depth, n_features, self.split_fn\n        )\n        ensemble_params.append(params)\n\n    # Setup optimizer\n    schedule = optax.cosine_decay_schedule(\n        config.learning_rate, config.epochs, alpha=0.01\n    )\n    optimizer = optax.adam(schedule)\n    opt_state = optimizer.init(ensemble_params)\n\n    # Training loop with early stopping\n    best_val_loss = float('inf')\n    best_params = ensemble_params\n    no_improve = 0\n\n    @jax.jit\n    def train_step(params, opt_state, X, y, temperature):\n        loss, grads = jax.value_and_grad(self._loss_fn)(\n            params, X, y, temperature, config\n        )\n        updates, opt_state = optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss\n\n    for epoch in range(config.epochs):\n        progress = epoch / config.epochs\n        temperature = config.temp_start + progress * (config.temp_end - config.temp_start)\n\n        ensemble_params, opt_state, train_loss = train_step(\n            ensemble_params, opt_state, X_train, y_train, temperature\n        )\n\n        # Validation check\n        if epoch % 10 == 0:\n            val_loss = self._loss_fn(\n                ensemble_params, X_val, y_val, temperature, config\n            )\n\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_params = ensemble_params\n                no_improve = 0\n            else:\n                no_improve += 10\n\n            if config.verbose and epoch % 50 == 0:\n                print(f\"  Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n\n            if no_improve &gt;= config.patience:\n                if config.verbose:\n                    print(f\"  Early stopping at epoch {epoch}\")\n                break\n\n    return TrainedGBM(\n        params=best_params,\n        config=config,\n        target_mean=target_mean,\n        target_std=target_std,\n        task=self.task,\n        split_fn=self.split_fn,\n        tree=self.tree,\n    )\n</code></pre>"},{"location":"api/training/#jaxboost.training.TrainerConfig","title":"TrainerConfig  <code>dataclass</code>","text":"<p>Configuration for gradient boosting training.</p> <p>Attributes:</p> Name Type Description <code>n_trees</code> <code>int</code> <p>Number of trees in the ensemble.</p> <code>depth</code> <code>int</code> <p>Maximum depth of each tree.</p> <code>learning_rate</code> <code>float</code> <p>Base learning rate for optimizer.</p> <code>tree_weight</code> <code>float</code> <p>Weight for each tree's contribution.</p> <code>epochs</code> <code>int</code> <p>Maximum number of training epochs.</p> <code>patience</code> <code>int</code> <p>Early stopping patience (epochs without improvement).</p> <code>temp_start</code> <code>float</code> <p>Starting temperature for soft routing.</p> <code>temp_end</code> <code>float | None</code> <p>Ending temperature (None = auto based on data size).</p> <code>val_fraction</code> <code>float</code> <p>Fraction of training data for validation (if no val set provided).</p> <code>normalize_target</code> <code>bool</code> <p>Whether to normalize regression targets.</p> <code>verbose</code> <code>bool</code> <p>Whether to print training progress.</p> Source code in <code>src/jaxboost/training/trainer.py</code> <pre><code>@dataclass\nclass TrainerConfig:\n    \"\"\"Configuration for gradient boosting training.\n\n    Attributes:\n        n_trees: Number of trees in the ensemble.\n        depth: Maximum depth of each tree.\n        learning_rate: Base learning rate for optimizer.\n        tree_weight: Weight for each tree's contribution.\n        epochs: Maximum number of training epochs.\n        patience: Early stopping patience (epochs without improvement).\n        temp_start: Starting temperature for soft routing.\n        temp_end: Ending temperature (None = auto based on data size).\n        val_fraction: Fraction of training data for validation (if no val set provided).\n        normalize_target: Whether to normalize regression targets.\n        verbose: Whether to print training progress.\n    \"\"\"\n    n_trees: int = 20\n    depth: int = 4\n    learning_rate: float = 0.01\n    tree_weight: float = 0.1\n    epochs: int = 500\n    patience: int = 50\n    temp_start: float = 1.0\n    temp_end: float | None = None  # Auto-detect\n    val_fraction: float = 0.15\n    normalize_target: bool = True\n    verbose: bool = False\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with JAXBoost.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install jaxboost\n</code></pre>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#regression","title":"Regression","text":"<pre><code>import jax.numpy as jnp\nfrom jaxboost import GBMTrainer\n\n# Generate sample data\nX_train = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\ny_train = jnp.array([1.0, 2.0, 3.0])\n\n# Create and fit trainer\ntrainer = GBMTrainer(task=\"regression\")\nmodel = trainer.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_train)\n</code></pre>"},{"location":"getting-started/quickstart/#binary-classification","title":"Binary Classification","text":"<pre><code>from jaxboost import GBMTrainer\n\ntrainer = GBMTrainer(task=\"binary_classification\")\nmodel = trainer.fit(X_train, y_train)\n\n# Get probability predictions\nprobabilities = model.predict_proba(X_train)\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Use <code>TrainerConfig</code> for advanced configuration:</p> <pre><code>from jaxboost import GBMTrainer, TrainerConfig\n\nconfig = TrainerConfig(\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n)\n\ntrainer = GBMTrainer(config=config, task=\"regression\")\n</code></pre>"},{"location":"getting-started/quickstart/#mixture-of-experts","title":"Mixture of Experts","text":"<p>For heterogeneous data with distinct patterns:</p> <pre><code>from jaxboost.ensemble import EMMOE, EMConfig, create_xgboost_expert\n\nexperts = [\n    create_xgboost_expert(task=\"regression\", n_estimators=100)\n    for _ in range(4)\n]\n\nconfig = EMConfig(num_experts=4, em_iterations=10)\nmoe = EMMOE(experts, config=config)\nmoe.fit(X_train, y_train)\n\npredictions = moe.predict(X_test)\n</code></pre> <p>See Ensemble API for more details.</p>"}]}